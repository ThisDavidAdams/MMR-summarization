{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MMR Summarisation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPq7l7J5XUQnMfshVkivokx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThisDavidAdams/MMR-summarization/blob/main/MMR_Summarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5z_cB7BqW8E"
      },
      "source": [
        "## Clone WCEP Repository and install dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSbPyLEkqGvw",
        "outputId": "83f0d756-6df3-43d6-b651-b6b08e921662"
      },
      "source": [
        "!git clone https://github.com/complementizer/wcep-mds-dataset\n",
        "%cd wcep-mds-dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'wcep-mds-dataset' already exists and is not an empty directory.\n",
            "/content/wcep-mds-dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCyycE6WqeIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f26d272-18d2-4951-c924-d15e206bc017"
      },
      "source": [
        "!pip install -r experiments/requirements.txt\n",
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn==0.23.1 in /usr/local/lib/python3.7/dist-packages (from -r experiments/requirements.txt (line 1)) (0.23.1)\n",
            "Requirement already satisfied: networkx==2.4 in /usr/local/lib/python3.7/dist-packages (from -r experiments/requirements.txt (line 2)) (2.4)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from -r experiments/requirements.txt (line 3)) (3.5)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r experiments/requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: newsroom from git+git://github.com/clic-lab/newsroom.git#egg=newsroom in /usr/local/lib/python3.7/dist-packages (from -r experiments/requirements.txt (line 5)) (0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1->-r experiments/requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1->-r experiments/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1->-r experiments/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx==2.4->-r experiments/requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r experiments/requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r experiments/requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r experiments/requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from newsroom->-r experiments/requirements.txt (line 5)) (1.1.5)\n",
            "Requirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from newsroom->-r experiments/requirements.txt (line 5)) (4.6.3)\n",
            "Requirement already satisfied: pyrouge>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from newsroom->-r experiments/requirements.txt (line 5)) (0.1.3)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.7/dist-packages (from newsroom->-r experiments/requirements.txt (line 5)) (4.0.2)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from newsroom->-r experiments/requirements.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: readability-lxml>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from newsroom->-r experiments/requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: spacy>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from newsroom->-r experiments/requirements.txt (line 5)) (2.2.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->newsroom->-r experiments/requirements.txt (line 5)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->newsroom->-r experiments/requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->newsroom->-r experiments/requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->newsroom->-r experiments/requirements.txt (line 5)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->newsroom->-r experiments/requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->newsroom->-r experiments/requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from readability-lxml>=0.6.2->newsroom->-r experiments/requirements.txt (line 5)) (4.2.6)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.7/dist-packages (from readability-lxml>=0.6.2->newsroom->-r experiments/requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (57.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (7.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->newsroom->-r experiments/requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (3.7.4.3)\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zZ0mzY5qlNd"
      },
      "source": [
        "## Download the test dataset\n",
        "\n",
        "WCEP-100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mWFuB0RqVfS",
        "outputId": "8a0c59ae-92e4-4eaf-9c2b-2c4691756958"
      },
      "source": [
        "!mkdir WCEP\n",
        "!gdown https://drive.google.com/uc?id=1qsd5pOCpeSXsaqNobXCrcAzhcjtG1wA1 -O WCEP/test.jsonl.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘WCEP’: File exists\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qsd5pOCpeSXsaqNobXCrcAzhcjtG1wA1\n",
            "To: /content/wcep-mds-dataset/WCEP/test.jsonl.gz\n",
            "51.5MB [00:01, 50.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqd0J5Yaq4Sy",
        "outputId": "6ea89a8d-9fb2-4b5b-a6c5-6498caf14c96"
      },
      "source": [
        "import experiments.utils as utils\n",
        "\n",
        "test_data = list(utils.read_jsonl_gz('WCEP/test.jsonl.gz'))\n",
        "# test_data = test_data[:25] # for testing\n",
        "print(\"Number of clusters:\",len(test_data))\n",
        "print(test_data[0].keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of clusters: 1022\n",
            "dict_keys(['id', 'date', 'reference_urls', 'articles', 'summary', 'wiki_links', 'category'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPMbjpSnjrK1",
        "outputId": "b645bc16-ca6a-4973-ff08-8a794557eac4"
      },
      "source": [
        "summary_max = 0\n",
        "article_max = 0\n",
        "for c in test_data:\n",
        "  summary_max = max(summary_max,len(c['summary'].split(\" \")))\n",
        "\n",
        "  for a in c['articles']:\n",
        "    if article_max < len(a['text'].split(\" \")):\n",
        "      text = a[\"text\"]\n",
        "    article_max = max(article_max,len(a['text'].split(\" \")))\n",
        "\n",
        "print(\"max length of articles\",article_max)\n",
        "print(\"max length of summary\",summary_max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max length of articles 13355\n",
            "max length of summary 119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vr7EC-QmBqO",
        "outputId": "43ea6613-acf5-469b-92cb-8840c894a23a"
      },
      "source": [
        "c = test_data[0]\n",
        "\n",
        "a = c['articles'][0]\n",
        "text1 = a['text']\n",
        "len(text1.split(\" \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxHK-XQ8xYKM"
      },
      "source": [
        "## Importing the pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-HyLgj4g_Xd",
        "outputId": "8971a378-2af2-45df-f2d6-d9d4c659eaa9"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "# !pip install spacy\n",
        "# !pip install bert-extractive-summarizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptT6ItiRoZzg",
        "outputId": "24955357-ad34-4e3a-b40f-de802fb1ee14"
      },
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5-dKDHGCWlZ"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FewxFm9gntW-"
      },
      "source": [
        "# PEGASUS\n",
        "\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "\n",
        "class Pegasus():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"Pegasus\"\n",
        "    self.model_name = \"google/pegasus-xsum\"\n",
        "    self.tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "    self.device = device\n",
        "    self.model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\").to(device)\n",
        "  \n",
        "  def summarise(self, text):\n",
        "    inputs = self.tokenizer([text], max_length=512, truncation = True, return_tensors='pt')\n",
        "    inputs.to(self.device)\n",
        "    translated = self.model.generate(inputs['input_ids'])\n",
        "    summary = self.tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "\n",
        "    return summary[0]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48miFfzMFetk"
      },
      "source": [
        "# T5\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "\n",
        "class T5():\n",
        "  \n",
        "  def __init__(self,device):\n",
        "    self.name = \"T5\"\n",
        "    self.device = device\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained('t5-large').to(device)\n",
        "    self.tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
        "\n",
        "  def summarise(self, text):\n",
        "    text = \"summarize: \" + text\n",
        "    tokenized_text = self.tokenizer.encode(text, return_tensors=\"pt\", truncation = True).to(self.device)\n",
        "\n",
        "    summary_ids = self.model.generate(tokenized_text,\n",
        "                                        num_beams=4,\n",
        "                                        no_repeat_ngram_size=2,\n",
        "                                        min_length=20,\n",
        "                                        max_length=50)\n",
        "    \n",
        "    summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wth4pvBTRRqm"
      },
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# GPT2\n",
        "class GPT2():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"GPT2\"\n",
        "    self.device = device\n",
        "    self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(self.device)\n",
        "    self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "  def summarise(self,text):\n",
        "    input_ids = self.tokenizer.encode(text, return_tensors = 'pt', truncation=True, max_length = 971)\n",
        "    tldr = self.tokenizer.encode(\" TL;DR:\", return_tensors = 'pt')\n",
        "    input_ids = torch.cat((input_ids,tldr),-1)\n",
        "    input_ids = input_ids.to(self.device)\n",
        "    beam_output = self.model.generate(\n",
        "        input_ids, \n",
        "        min_length = len(input_ids[0]) + 20,\n",
        "        max_length=len(input_ids[0]) + 50, \n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2, \n",
        "        temperature=0.5,\n",
        "        early_stopping=True,\n",
        "        top_p = 0.9\n",
        "    )\n",
        "    output = self.tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "    \n",
        "    summary = output.split(\"TL;DR:\")[-1]\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9tlGNLIvPya"
      },
      "source": [
        "# XLNet\n",
        "\n",
        "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
        "\n",
        "class XLNet():\n",
        "  \n",
        "  def __init__(self,device):\n",
        "    self.name = \"XLNet\"\n",
        "    self.device = device\n",
        "    self.tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
        "    self.model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased').to(device)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    input_ids = self.tokenizer.encode(text, return_tensors = 'pt', truncation=True, max_length = 971)\n",
        "    tldr = self.tokenizer.encode(\" TL;DR:\", return_tensors = 'pt')\n",
        "    input_ids = torch.cat((input_ids,tldr),-1)\n",
        "    input_ids = input_ids.to(self.device)\n",
        "    beam_output = self.model.generate(input_ids, \n",
        "                                max_length=len(input_ids[0]) + 50, \n",
        "                                num_beams=5,\n",
        "                                no_repeat_ngram_size=2, \n",
        "                                temperature=0.7,\n",
        "                                early_stopping=True,\n",
        "                                top_p = 0.9\n",
        "                                )\n",
        "    \n",
        "    output = self.tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "    output = output.split(\"TL;DR:\")[-1]\n",
        "    return output\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdBfYdER-B7V"
      },
      "source": [
        "# ProphetNet\n",
        "\n",
        "from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration\n",
        "\n",
        "class ProphetNet():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"ProphetNet\"\n",
        "    self.device = device\n",
        "    self.tokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased')\n",
        "    self.model = ProphetNetForConditionalGeneration.from_pretrained('microsoft/prophetnet-large-uncased').to(self.device)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    input_ids = self.tokenizer(text, return_tensors=\"pt\", truncation = True).input_ids\n",
        "    decoder_input_ids = self.tokenizer(\"To summarise\", return_tensors=\"pt\").input_ids  \n",
        "    input_ids = input_ids.to(self.device)\n",
        "    decoder_input_ids = decoder_input_ids.to(self.device)\n",
        "\n",
        "    beam_output = self.model.generate(input_ids, \n",
        "                                decoder_input_ids = decoder_input_ids,\n",
        "                                max_length= 50, \n",
        "                                num_beams=5,\n",
        "                                no_repeat_ngram_size=2, \n",
        "                                temperature=0.7,\n",
        "                                early_stopping=True,\n",
        "                                top_p = 0.9\n",
        "                                )\n",
        "    \n",
        "    output = self.tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMindhh-hsvu"
      },
      "source": [
        "# BART\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "\n",
        "class BART():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"BART\"\n",
        "    self.device = device\n",
        "    self.model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
        "    self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "  def summarise(self, text, MDS = False):\n",
        "    inputs = self.tokenizer([text], max_length=1024, truncation = True, return_tensors='pt').to(self.device)\n",
        "    summary_ids = self.model.generate(inputs['input_ids'], num_beams=4, max_length=40, early_stopping=True)\n",
        "    return ' '.join([(self.tokenizer).decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lon9v6ReoddZ"
      },
      "source": [
        "# LED\n",
        "\n",
        "from transformers import LEDTokenizer, LEDForConditionalGeneration, LEDConfig\n",
        "\n",
        "class LED():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"LED\"\n",
        "    self.device = device\n",
        "    self.model = LEDForConditionalGeneration.from_pretrained('allenai/led-base-16384').to(self.device)\n",
        "    self.tokenizer = LEDTokenizer.from_pretrained('allenai/led-base-16384')\n",
        "\n",
        "  def summarise(self, text, MDS = False):\n",
        "    inputs = self.tokenizer([text], max_length=1024, truncation=True, return_tensors='pt').to(self.device)\n",
        "    summary_ids = self.model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n",
        "    return ' '.join([(self.tokenizer).decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vTp6ExAp-Ks"
      },
      "source": [
        "prophetnet = ProphetNet(device)\n",
        "gpt2 = GPT2(device)\n",
        "xlnet = XLNet(device)\n",
        "t5 = T5(device)\n",
        "led = LED(device)\n",
        "bart = BART(device)\n",
        "pegasus = Pegasus(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Nv7bOVitwS"
      },
      "source": [
        "SDS_models = {\n",
        "      \"ProfetNet\" : prophetnet,\n",
        "      \"GPT2\" : gpt2,\n",
        "      \"T5\" : t5,\n",
        "      \"LED\" : led,\n",
        "      \"BART\" : bart,\n",
        "      \"PEGASUS\" : pegasus,\n",
        "      \"XLNet\" : xlnet\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv0OHoeN-Q4U",
        "outputId": "0c8ba5d3-399e-425d-ae6a-a22e1b2aa1bd"
      },
      "source": [
        "for _,model in SDS_models.items():\n",
        "  print(model.name,\"\\n\")\n",
        "  print(model.summarise(text))\n",
        "  print(\"-*-\"*10,\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProphetNet \n",
            "\n",
            "to summarise the president ’ s week in iowa, sitting down for an exclusive interview with president donald trump, embedding him in the oval office for a day of meetings, greeting him for the first time in his life, and sitting\n",
            "-*--*--*--*--*--*--*--*--*--*- \n",
            "\n",
            "GPT2 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " If you want to be able to keep your health insurance, then you need to go through the process of getting a court order, which is a process that takes a long time. The process is very, very complicated. There are a number of things\n",
            "-*--*--*--*--*--*--*--*--*--*- \n",
            "\n",
            "T5 \n",
            "\n",
            "president's 30-hour visit to the white house aired on \"20/20\" on sunday, july 16. in the interview, president says he feels like this is his \"great part of the country\" president\n",
            "-*--*--*--*--*--*--*--*--*--*- \n",
            "\n",
            "LED \n",
            "\n",
            "Over the course of two days, ABC News' chief anchor George Stephanopoulos spent 30 hours with President Donald Trump, flying on Air Force One to Iowa, traveling in his armored vehicle called “The Beast,” greeting him in his\n",
            "-*--*--*--*--*--*--*--*--*--*- \n",
            "\n",
            "BART \n",
            "\n",
            "\"President Trump - 30 Hours\" airs on Sunday, June 16, 2019, at 8 p.m. ET. George Stephanopoulos spent 30 hours with President Donald Trump in Iowa.\n",
            "-*--*--*--*--*--*--*--*--*--*- \n",
            "\n",
            "Pegasus \n",
            "\n",
            "President Donald Trump sat down for an exclusive interview with ABC News' chief anchor George Stephanopoulos following his second-place finish in the Iowa caucuses.\n",
            "-*--*--*--*--*--*--*--*--*--*- \n",
            "\n",
            "XLNet \n",
            "\n",
            ": We won the Senate. The Senate is a big issue. TA DA: I don’t know what to think about that. There are so many things that are going on in this country. That is just a little bit\n",
            "-*--*--*--*--*--*--*--*--*--*- \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8HkVc91jb9l"
      },
      "source": [
        "## Setting up LDAMallet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PcpmeuIjfO-"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2jovFPFjggG"
      },
      "source": [
        "# install JAVA\n",
        "\n",
        "import os       \n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujgB_I-Cjn70"
      },
      "source": [
        "# Install Mallet\n",
        "\n",
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6IwJjiQiRHW"
      },
      "source": [
        "## Compute MMR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5l-biTniUFv"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def maximal_marginal_relevance(sentence_vector, phrases, embedding_matrix, lambda_constant=0.5, threshold_terms=10):\n",
        "    \"\"\"\n",
        "    Return ranked phrases using MMR. Cosine similarity is used as similarity measure.\n",
        "    :param sentence_vector: Query vector\n",
        "    :param phrases: list of candidate phrases\n",
        "    :param embedding_matrix: matrix having index as phrases and values as vector\n",
        "    :param lambda_constant: 0.5 to balance diversity and accuracy. if lambda_constant is high, then higher accuracy. If lambda_constant is low then high diversity.\n",
        "    :param threshold_terms: number of terms to include in result set\n",
        "    :return: Ranked phrases with score\n",
        "    \"\"\"\n",
        "    # todo: Use cosine similarity matrix for lookup among phrases instead of making call everytime.\n",
        "    s = []\n",
        "    r = sorted(phrases, key=lambda x: x[1], reverse=True)\n",
        "    r = [i[0] for i in r]\n",
        "    while len(r) > 0:\n",
        "        score = 0\n",
        "        phrase_to_add = ''\n",
        "        for i in r:\n",
        "            first_part = cosine_similarity([sentence_vector], [embedding_matrix.loc[i]])[0][0]\n",
        "            second_part = 0\n",
        "            for j in s:\n",
        "                cos_sim = cosine_similarity([embedding_matrix.loc[i]], [embedding_matrix.loc[j[0]]])[0][0]\n",
        "                if cos_sim > second_part:\n",
        "                    second_part = cos_sim\n",
        "            equation_score = lambda_constant*(first_part)-(1-lambda_constant) * second_part\n",
        "            if equation_score > score:\n",
        "                score = equation_score\n",
        "                phrase_to_add = i\n",
        "        if phrase_to_add == '':\n",
        "            phrase_to_add = i\n",
        "        r.remove(phrase_to_add)\n",
        "        s.append((phrase_to_add, score))\n",
        "    return (s, s[:threshold_terms])[threshold_terms > len(s)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0obC0aYidBsz"
      },
      "source": [
        "## Generate Summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2BqJuUkiaNJ"
      },
      "source": [
        "from experiments.evaluate import evaluate\n",
        "from tqdm import tqdm \n",
        "import pickle\n",
        "\n",
        "# Generating summaries\n",
        "\n",
        "summaries = []\n",
        "# summary keys : summary, type (MDS/SDS), index (None/article_index), model, clusterid, Rouge_score, MMR_reduced\n",
        "\n",
        "for cluster in tqdm(test_data):\n",
        "\n",
        "  # MDS\n",
        "  for model in MDS_models:\n",
        "    d = {}\n",
        "\n",
        "    d[\"type\"] = \"MDS\"\n",
        "    d[\"model\"] = model\n",
        "    d[\"index\"] = None\n",
        "    d[\"clusterId\"] = clusterId\n",
        "\n",
        "    summary = MDS_models[model](cluster['articles'])\n",
        "    d[\"summary\"] = summary\n",
        "\n",
        "    d[\"rouge\"] = evaluate([summary], [cluster[\"summary\"]])\n",
        "    d[\"MMR_reduced\"] = None\n",
        "    summaries.append(d)\n",
        "  \n",
        "  # SDS\n",
        "  for model in SDS_models:\n",
        "    for article in cluster['articles']:\n",
        "      d = {}\n",
        "\n",
        "      d[\"type\"] = \"SDS\"\n",
        "      d[\"model\"] = model\n",
        "      d[\"index\"] = article[\"id\"]\n",
        "      d[\"clusterId\"] = clusterId\n",
        "\n",
        "      summary = SDS_models[model](article)\n",
        "      d[\"summary\"] = summary\n",
        "\n",
        "      d[\"rouge\"] = evaluate([summary], [cluster[\"summary\"]])\n",
        "      d[\"MMR_reduced\"] = None\n",
        "      summaries.append(d)\n",
        "\n",
        "# save the progress\n",
        "utils.write_jsonl(summaries, \"summaries.jsonl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMixsrhzrXIJ"
      },
      "source": [
        "summaries = list(utils.read_jsonl(\"summaries.jsonl\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyh35cmqdFIQ"
      },
      "source": [
        "## MMR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAXUhIPAsgYG"
      },
      "source": [
        "import experiments.sent_splitter as sent_splitter\n",
        "\n",
        "sentSplitter = sent_splitter.SentenceSplitter()\n",
        "\n",
        "# MMR 1\n",
        "for summ in summaries:\n",
        "\n",
        "  text = summ[\"summary\"]\n",
        "  sentences = sentSplitter(text)\n",
        "\n",
        "  # MMR\n",
        "  output = None # MMR ranked - least diverse\n",
        "  summ[\"MMR_reduced\"] = output\n",
        "\n",
        "# save the progress\n",
        "utils.write_jsonl(summaries, \"summaries.jsonl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3_ZnDA6ulB5"
      },
      "source": [
        "summaries = list(utils.read_jsonl(\"summaries.jsonl\"))\n",
        "\n",
        "clusters = [c[\"clusterId\"] for c in summaries]\n",
        "clusters = list(set(clusters))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-rfQaFrdJNU"
      },
      "source": [
        "## Final Summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YRf6GxHqWme"
      },
      "source": [
        "# Generating final summary\n",
        "# MMR 2\n",
        "\n",
        "final_summaries = []\n",
        "\n",
        "for cid in clusters:\n",
        "  d = {}\n",
        "  d[\"clusterId\"] = cid\n",
        "\n",
        "  cluster_summaries = [c for c in summaries if cid = c[\"clusterId\"]]\n",
        "  ground_truth = [c[\"summary\"] for c in test_data where c[\"id\"] == cid][0]\n",
        "\n",
        "  cluster_sentences = []\n",
        "  for summ in cluster_summaries:\n",
        "\n",
        "    text = summ[\"MMR_reduced\"]\n",
        "    sentences = sentSplitter(text)\n",
        "\n",
        "    cluster_sentences.extend(sentences)\n",
        "\n",
        "\n",
        "  # MMR \n",
        "\n",
        "  # pick top n sentences\n",
        "  final_summary = ' '.join(topn_sentences)\n",
        "  Rouge_score = evaluate([final_summary], [ground_truth])\n",
        "\n",
        "  d[\"summary\"] = final_summary\n",
        "  d[\"rouge\"] = Rouge_score\n",
        "\n",
        "\n",
        "  final_summaries.append(d)\n",
        "\n",
        "# save the progress  \n",
        "utils.write_jsonl(final_summaries, \"finalsummaries.jsonl\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}