{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MMR Summarisation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOTVxlBS/qWj0DGmGP0uFKe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThisDavidAdams/MMR-summarization/blob/main/MMR_Summarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8877kYHgFAza"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5z_cB7BqW8E"
      },
      "source": [
        "## Clone WCEP Repository and install dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSbPyLEkqGvw"
      },
      "source": [
        "!git clone https://github.com/gandharvsuri/wcep-mds-dataset\n",
        "%cd wcep-mds-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCyycE6WqeIY"
      },
      "source": [
        "!pip install -r experiments/requirements.txt\n",
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zZ0mzY5qlNd"
      },
      "source": [
        "## Download the test dataset\n",
        "\n",
        "WCEP-100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mWFuB0RqVfS"
      },
      "source": [
        "!mkdir WCEP\n",
        "# !gdown https://drive.google.com/uc?id=1qsd5pOCpeSXsaqNobXCrcAzhcjtG1wA1 -O WCEP/test.jsonl.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqd0J5Yaq4Sy",
        "outputId": "8a483f68-2c89-4ce8-aa18-83b23bf80dbc"
      },
      "source": [
        "import experiments.utils as utils\n",
        "\n",
        "test_data = list(utils.read_jsonl('/content/gdrive/MyDrive/ULETH/test_data.jsonl'))\n",
        "partial_test_data = test_data[:10]\n",
        "print(\"Number of clusters:\",len(test_data))\n",
        "print(test_data[0].keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of clusters: 500\n",
            "dict_keys(['id', 'date', 'reference_urls', 'articles', 'summary', 'wiki_links', 'category'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPMbjpSnjrK1"
      },
      "source": [
        "summary_max = 0\n",
        "article_max = 0\n",
        "for c in test_data:\n",
        "  summary_max = max(summary_max,len(c['summary'].split(\" \")))\n",
        "\n",
        "  for a in c['articles']:\n",
        "    if article_max < len(a['text'].split(\" \")):\n",
        "      long_text = a[\"text\"]\n",
        "    article_max = max(article_max,len(a['text'].split(\" \")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR6bmVdKoejH"
      },
      "source": [
        "test_data = test_data[:500]\n",
        "assert len(test_data) == 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twaUMwN-igpD"
      },
      "source": [
        "from statistics import mean\n",
        "import experiments.sent_splitter as sent_splitter\n",
        "\n",
        "sentSplitter = sent_splitter.SentenceSplitter()\n",
        "\n",
        "article_word_count = [len(a['text'].split()) for c in test_data for a in c[\"articles\"]]\n",
        "summary_word_count = [len(c['summary'].split()) for c in test_data]\n",
        "summary_sent_length = [len(sentSplitter.split_sents(c[\"summary\"])) for c in test_data]\n",
        "\n",
        "print(\"max word count of articles:\",max(article_word_count))\n",
        "print(\"max word count of summary:\",max(summary_word_count))\n",
        "print(\"avg word count of articles:\", int(mean(article_word_count)))\n",
        "print(\"avg word count of summary:\", int(mean(summary_word_count)))\n",
        "print(\"max sent count of summary:\",max(summary_sent_length))\n",
        "print(\"avg sent count of summary:\",int(mean(summary_sent_length)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ks1JlK15eOU3",
        "outputId": "968df672-380c-4475-8cbf-70804c5e1e0f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(1, 2, 1)  \n",
        "plt.hist(article_word_count, bins = 500)\n",
        "\n",
        "plt.subplot(1, 2, 2)  \n",
        "plt.hist(summary_word_count)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMthc1ikgLTL"
      },
      "source": [
        "max_summary_length = 150\n",
        "min_summary_length = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxHK-XQ8xYKM"
      },
      "source": [
        "## Importing the pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-HyLgj4g_Xd"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install spacy\n",
        "!pip install bert-extractive-summarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUtHoaZX07UL"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptT6ItiRoZzg",
        "outputId": "390b7868-0288-464c-b8f8-558be79a9d55"
      },
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5-dKDHGCWlZ"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TuZpgzJDz-T"
      },
      "source": [
        "#### Abstractive Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FewxFm9gntW-"
      },
      "source": [
        "# PEGASUS\n",
        "\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "\n",
        "class Pegasus():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"Pegasus\"\n",
        "    self.model_name = \"google/pegasus-xsum\"\n",
        "    self.tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "    self.device = device\n",
        "    self.model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\").to(device)\n",
        "  \n",
        "  def summarise(self, text):\n",
        "    inputs = self.tokenizer([text], max_length=512, truncation = True, return_tensors='pt').to(self.device)\n",
        "    translated = self.model.generate(inputs['input_ids'],\n",
        "                                        min_length=min_summary_length,\n",
        "                                        max_length=max_summary_length,\n",
        "                                        temperature=0.6,\n",
        "                                        early_stopping=True,\n",
        "                                        top_k = 50,\n",
        "                                        no_repeat_ngram_size= 4,\n",
        "                                    )\n",
        "    \n",
        "    summary = self.tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "    del inputs\n",
        "    del translated\n",
        "\n",
        "    return summary[0]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48miFfzMFetk"
      },
      "source": [
        "# T5\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "\n",
        "class T5():\n",
        "  \n",
        "  def __init__(self,device):\n",
        "    self.name = \"T5\"\n",
        "    self.device = device\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained('t5-large').to(device)\n",
        "    self.tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
        "\n",
        "  def summarise(self, text):\n",
        "    text = \"summarize: \" + text\n",
        "    tokenized_text = self.tokenizer.encode(text, return_tensors=\"pt\", truncation = True).to(self.device)\n",
        "\n",
        "    summary_ids = self.model.generate(tokenized_text,\n",
        "                                        min_length=min_summary_length,\n",
        "                                        max_length=max_summary_length,\n",
        "                                        temperature=0.7,\n",
        "                                        early_stopping=True,\n",
        "                                        top_k = 50,\n",
        "                                        no_repeat_ngram_size= 4,\n",
        "                                      )\n",
        "    \n",
        "    summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    del text\n",
        "    del tokenized_text\n",
        "    del summary_ids\n",
        "    return summary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wth4pvBTRRqm"
      },
      "source": [
        "# GPT2\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "class GPT2():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"GPT2\"\n",
        "    self.device = device\n",
        "    self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(self.device)\n",
        "    self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "  def summarise(self,text):\n",
        "    input_ids = self.tokenizer.encode(text, return_tensors = 'pt', truncation=True, max_length = 1020 - max_summary_length)\n",
        "    tldr = self.tokenizer.encode(\" TL;DR:\", return_tensors = 'pt')\n",
        "    input_ids = torch.cat((input_ids,tldr),-1)\n",
        "    input_ids = input_ids.to(self.device)\n",
        "    beam_output = self.model.generate(input_ids, \n",
        "                                      min_length = len(input_ids[0]) + min_summary_length,\n",
        "                                      max_length=len(input_ids[0]) + max_summary_length,\n",
        "                                      temperature=0.6,\n",
        "                                      early_stopping=True,\n",
        "                                      top_k = 50,\n",
        "                                      no_repeat_ngram_size= 4,\n",
        "                                    )\n",
        "    output = self.tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "    \n",
        "    summary = output.split(\"TL;DR:\")[-1]\n",
        "    del input_ids\n",
        "    del tldr\n",
        "    del beam_output\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9tlGNLIvPya"
      },
      "source": [
        "# XLNet\n",
        "\n",
        "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
        "\n",
        "class XLNet():\n",
        "  \n",
        "  def __init__(self,device):\n",
        "    self.name = \"XLNet\"\n",
        "    self.device = device\n",
        "    self.tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
        "    self.model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased').to(device)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    input_ids = self.tokenizer.encode(text, return_tensors = 'pt', truncation=True, max_length = 1020 - max_summary_length)\n",
        "    tldr = self.tokenizer.encode(\" TL;DR:\", return_tensors = 'pt')\n",
        "    input_ids = torch.cat((input_ids,tldr),-1)\n",
        "    input_ids = input_ids.to(self.device)\n",
        "    beam_output = self.model.generate(input_ids,\n",
        "                                      min_length=len(input_ids[0]) + min_summary_length,\n",
        "                                      max_length=len(input_ids[0]) + max_summary_length,\n",
        "                                      temperature=0.6,\n",
        "                                      early_stopping=True,\n",
        "                                      top_k = 50,\n",
        "                                      no_repeat_ngram_size= 4,\n",
        "                                    )\n",
        "    \n",
        "    output = self.tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "    output = output.split(\"TL;DR:\")[-1]\n",
        "    del input_ids\n",
        "    del tldr\n",
        "    del beam_output\n",
        "    \n",
        "    return output\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdBfYdER-B7V"
      },
      "source": [
        "# ProphetNet\n",
        "\n",
        "from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration\n",
        "\n",
        "class ProphetNet():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"ProphetNet\"\n",
        "    self.device = device\n",
        "    self.tokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased')\n",
        "    self.model = ProphetNetForConditionalGeneration.from_pretrained('microsoft/prophetnet-large-uncased').to(self.device)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    input_ids = self.tokenizer(text, return_tensors=\"pt\", truncation = True).input_ids\n",
        "    decoder_input_ids = self.tokenizer(\"To summarise\", return_tensors=\"pt\").input_ids  \n",
        "    input_ids = input_ids.to(self.device)\n",
        "    decoder_input_ids = decoder_input_ids.to(self.device)\n",
        "\n",
        "    beam_output = self.model.generate(input_ids, \n",
        "                                      decoder_input_ids = decoder_input_ids,\n",
        "                                      min_length = min_summary_length,\n",
        "                                      max_length = max_summary_length,\n",
        "                                      temperature=0.7,\n",
        "                                      early_stopping=True,\n",
        "                                      top_k = 50,\n",
        "                                      no_repeat_ngram_size= 4,\n",
        "                                )\n",
        "    \n",
        "    output = self.tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "    del input_ids\n",
        "    del decoder_input_ids\n",
        "    del beam_output\n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMindhh-hsvu"
      },
      "source": [
        "# BART\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "\n",
        "class BART():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"BART\"\n",
        "    self.device = device\n",
        "    self.model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
        "    self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "  def summarise(self, text, MDS = False):\n",
        "    inputs = self.tokenizer([text], max_length=1024, truncation = True, return_tensors='pt').to(self.device)\n",
        "    summary_ids = self.model.generate(inputs['input_ids'], \n",
        "                                      min_length = min_summary_length,\n",
        "                                      max_length = max_summary_length,\n",
        "                                      temperature=0.7,\n",
        "                                      early_stopping=True,\n",
        "                                      top_k = 50,\n",
        "                                      no_repeat_ngram_size= 4,\n",
        "                                    )\n",
        "    \n",
        "    summary = ' '.join([(self.tokenizer).decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
        "    del inputs\n",
        "    del summary_ids\n",
        "\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lon9v6ReoddZ"
      },
      "source": [
        "# LED\n",
        "\n",
        "from transformers import LEDTokenizer, LEDForConditionalGeneration, LEDConfig\n",
        "\n",
        "class LED():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"LED\"\n",
        "    self.device = device\n",
        "    self.model = LEDForConditionalGeneration.from_pretrained('allenai/led-base-16384').to(self.device)\n",
        "    self.tokenizer = LEDTokenizer.from_pretrained('allenai/led-base-16384')\n",
        "\n",
        "  def summarise(self, text, MDS = False):\n",
        "    inputs = self.tokenizer([text], max_length=1024, truncation=True, return_tensors='pt').to(self.device)\n",
        "    summary_ids = self.model.generate(inputs['input_ids'], \n",
        "                                      min_length = min_summary_length,\n",
        "                                      max_length = max_summary_length,\n",
        "                                      temperature=0.7,\n",
        "                                      early_stopping=True,\n",
        "                                      top_k = 50,\n",
        "                                      no_repeat_ngram_size= 4,\n",
        "                                    )\n",
        "    summary = ' '.join([(self.tokenizer).decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
        "    del inputs\n",
        "    del summary_ids\n",
        "\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsp2KkfmD59V"
      },
      "source": [
        "#### Extractive Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z28SohN-zGU"
      },
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
        "from summarizer import Summarizer\n",
        "# GPT2 extractive\n",
        "\n",
        "class GPT2_ext():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.name = \"GPT2 EXT\"\n",
        "    model_name = \"gpt2\"\n",
        "    custom_config = AutoConfig.from_pretrained(model_name)\n",
        "    custom_config.output_hidden_states=True\n",
        "    custom_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    custom_model = AutoModel.from_pretrained(model_name, config=custom_config)\n",
        "    self.model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    return self.model(text,ratio = 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU3o9cSD-0NH"
      },
      "source": [
        "# XLNet extractive\n",
        "\n",
        "class XLNet_ext():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.name = \"XLNet EXT\"\n",
        "    model_name = \"xlnet-large-cased\"\n",
        "    custom_config = AutoConfig.from_pretrained(model_name)\n",
        "    custom_config.output_hidden_states=True\n",
        "    custom_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    custom_model = AutoModel.from_pretrained(model_name, config=custom_config)\n",
        "    self.model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    return self.model(text,ratio = 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYAji204_iMu"
      },
      "source": [
        "# BART extractive\n",
        "\n",
        "class BART_ext():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.name = \"BART EXT\"\n",
        "    model_name = \"facebook/bart-large-cnn\"\n",
        "    custom_config = AutoConfig.from_pretrained(model_name)\n",
        "    custom_config.output_hidden_states=True\n",
        "    custom_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    custom_model = AutoModel.from_pretrained(model_name, config=custom_config)\n",
        "    self.model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    return self.model(text,ratio = 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoinV-r3XqxE"
      },
      "source": [
        "## Generate Summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kceh3_OG_4-m"
      },
      "source": [
        "%%time \n",
        "\n",
        "prophetnet = ProphetNet(device)\n",
        "gpt2 = GPT2(device)\n",
        "xlnet = XLNet(device)\n",
        "t5 = T5(device)\n",
        "led = LED(device)\n",
        "bart = BART(device)\n",
        "pegasus = Pegasus(device)\n",
        "\n",
        "gpt2_ext = GPT2_ext()\n",
        "xlnet_ext = XLNet_ext()\n",
        "# t5_ext = T5_ext()\n",
        "# led_ext = LED_ext()\n",
        "bart_ext = BART_ext()\n",
        "# pegasus_ext = Pegasus_ext()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-pu1tps__RY"
      },
      "source": [
        "models = {\n",
        "      \"ProphetNet\" : prophetnet,\n",
        "      \"GPT2\" : gpt2,\n",
        "      \"T5\" : t5,\n",
        "      \"XLNet\" : xlnet,\n",
        "      \"LED\" : led,\n",
        "      \"BART\" : bart,\n",
        "      \"Pegasus\" : pegasus,\n",
        "      \"GPT2 EXT\": gpt2_ext,\n",
        "      \"XLNet EXT\": xlnet_ext,\n",
        "      \"BART EXT\": bart_ext,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGb_PK6k1Y5M"
      },
      "source": [
        "from experiments.evaluate import evaluate\n",
        "from tqdm import tqdm \n",
        "import os.path\n",
        "import gc\n",
        "import re\n",
        "from datetime import datetime\n",
        "# Generating summaries\n",
        "\n",
        "def generateBaselineSummaries(test_data, num_articles, models = models)\n",
        "  path = \"/content/gdrive/MyDrive/ULETH/summaries_partial.jsonl\"\n",
        "  summaries = list(utils.read_jsonl(path)) if os.path.isfile(path) else []\n",
        "  start = len(summaries)\n",
        "  # summary keys : summary, type (MDS/SDS), index (None/article_index), model, clusterid, Rouge_score, MMR_reduced\n",
        "\n",
        "  print(\"Starting from {} cluster\".format(start))\n",
        "\n",
        "  for cluster in partial_test_data[start:]:\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    c = {} \n",
        "    c[\"cluster_id\"] = cluster[\"id\"]\n",
        "    c[\"summaries\"] = {}\n",
        "\n",
        "    for article in cluster['articles'][:num_articles]:\n",
        "      c[\"summaries\"][article[\"id\"]] = []\n",
        "\n",
        "    for _,model in models.items():\n",
        "      for article in cluster['articles'][:num_articles]:\n",
        "\n",
        "        #Clean up CUDA\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        d = {}\n",
        "        text = article[\"text\"]\n",
        "        if text == '':\n",
        "          continue\n",
        "\n",
        "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        summary = model.summarise(text)\n",
        "\n",
        "        d[\"type\"] = \"SDS\"\n",
        "        d[\"model\"] = model.name\n",
        "\n",
        "        d[\"summary\"] = summary\n",
        "\n",
        "        rouge = evaluate([summary], [cluster[\"summary\"]])\n",
        "        d[\"rouge\"] = rouge\n",
        "        c[\"summaries\"][article[\"id\"]].append(d)\n",
        "\n",
        "        del summary\n",
        "        del d\n",
        "        del text\n",
        "        del rouge\n",
        "\n",
        "    summaries.append(c) \n",
        "    del c\n",
        "\n",
        "    utils.write_jsonl(summaries, path=path, override = True)\n",
        "  return summaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWeHl78J-6Gw"
      },
      "source": [
        "summaries = generateBaselineSummaries(test_data, num_articles = 10, models = models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCETJ0OsXzY1"
      },
      "source": [
        "### Baseline models score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7y6mSig8yio"
      },
      "source": [
        "summary_results = {}\n",
        "\n",
        "model_names = set()\n",
        "\n",
        "for d in summaries:\n",
        "  for _, summ in d[\"summaries\"].items():\n",
        "    for s in summ:\n",
        "      model_names.add(s[\"model\"])\n",
        "\n",
        "model_names = list(model_names)\n",
        "\n",
        "for m in model_names:\n",
        "  summary_results[m] = []\n",
        "\n",
        "for d in summaries:\n",
        "  for _, summ in d[\"summaries\"].items():\n",
        "    for s in summ:\n",
        "      summary_results[s[\"model\"]].append(s[\"rouge\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHFCd9D4D_GJ"
      },
      "source": [
        "for model,scores in summary_results.items():\n",
        "  r1 = []\n",
        "  r2 = []\n",
        "  rl = []\n",
        "\n",
        "  for s in scores:\n",
        "    r1.append(s[\"rouge-1\"])\n",
        "    r2.append(s[\"rouge-2\"])\n",
        "    rl.append(s[\"rouge-l\"])\n",
        "  \n",
        "  r1p = mean([x['p'] for x in r1]) *100\n",
        "  r1r = mean([x['r'] for x in r1]) *100\n",
        "  r1f = mean([x['f'] for x in r1]) *100\n",
        "  \n",
        "  r2p = mean([x['p'] for x in r2]) *100\n",
        "  r2r = mean([x['r'] for x in r2]) *100\n",
        "  r2f = mean([x['f'] for x in r2]) *100\n",
        "  \n",
        "  rlp = mean([x['p'] for x in rl]) *100\n",
        "  rlr = mean([x['r'] for x in rl]) *100\n",
        "  rlf = mean([x['f'] for x in rl]) *100\n",
        "  \n",
        "  print(model)\n",
        "  print(\"r1 p: {:.3f} r: {:.3f} f: {:.3f} \\nr2 p: {:.3f} r: {:.3f} f: {:.3f} \\nr2 p: {:.3f} r: {:.3f} f: {:.3f} \\n\".format(r1p,r1r,r1f,r2p,r2r,r2p,rlp,rlr,rlf))\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8HkVc91jb9l"
      },
      "source": [
        "## Setting up LDAMallet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wy8zn2Cz575"
      },
      "source": [
        "### Load summaires"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMVO9W_dzt9f"
      },
      "source": [
        "import os.path\n",
        "path = \"/content/gdrive/MyDrive/ULETH/summaries_final.jsonl\"\n",
        "summaries = list(utils.read_jsonl(path)) if os.path.isfile(path) else []\n",
        "assert len(summaries) == 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSYKu5C50EOg"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PcpmeuIjfO-"
      },
      "source": [
        "!pip install gensim==3.8.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2jovFPFjggG"
      },
      "source": [
        "# install JAVA\n",
        "import os       \n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujgB_I-Cjn70"
      },
      "source": [
        "# Install Mallet\n",
        "# !rm -rf /content/wcep-mds-dataset/mallet-2.0.8\n",
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoJnf4b3ojD1"
      },
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.wrappers import LdaMallet\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim import similarities\n",
        "\n",
        "import os.path\n",
        "import re\n",
        "import glob\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_Q_47OpomG6"
      },
      "source": [
        "def preprocess_data(doc_set, extra_stopwords = {}):\n",
        "    # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
        "    # replace all newlines or multiple sequences of spaces with a standard space\n",
        "\n",
        "    # new_words = []\n",
        "    # for sentence in sentences:\n",
        "    #     new_words.append(gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
        "\n",
        "    # # Build the bigram and trigram models\n",
        "    # bigram = gensim.models.Phrases(new_words, min_count=5, threshold=100)  # higher threshold fewer phrases.\n",
        "    # trigram = gensim.models.Phrases(bigram[new_words], threshold=100)\n",
        "    \n",
        "    # # Faster way to get a sentence clubbed as a trigram/bigram\n",
        "    \n",
        "    # bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    # trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "    doc_set = [re.sub('\\s+', ' ', doc) for doc in doc_set]\n",
        "    doc_set = [doc.replace('\\n', ' ') for doc in doc_set]\n",
        "    doc_set = [doc.replace(\"'\", \"\") for doc in doc_set]\n",
        "    doc_set = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', doc, flags=re.MULTILINE) for doc in doc_set]\n",
        "\n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create English stop words list\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    # add any extra stopwords\n",
        "    if (len(extra_stopwords) > 0):\n",
        "        en_stop = en_stop.union(extra_stopwords)\n",
        "    \n",
        "    # list for tokenized documents in loop\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "    for i in doc_set:\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        # add tokens to list\n",
        "        texts.append(stopped_tokens)\n",
        "    return texts\n",
        "\n",
        "def prepare_corpus(doc_clean):\n",
        "    # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
        "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # generate LDA model\n",
        "    return dictionary,doc_term_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEJ5TFE7w-31"
      },
      "source": [
        "def remove_stopwords(texts):\n",
        "  return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts, bigram_mod):\n",
        "  return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts, bigram_mod):\n",
        "  return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(docs, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "  \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "  texts_out = []\n",
        "  for sent in texts:\n",
        "      doc = nlp(\" \".join(sent))\n",
        "      texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "  return texts_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InBUAGcz0H8R"
      },
      "source": [
        "### Generate LDA topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl0_VFARqEfY"
      },
      "source": [
        "import gc\n",
        "from tqdm import tqdm\n",
        "import os.path\n",
        "\n",
        "def LDA_topics(test_data, num_topics, num_words, num_articles = 10):\n",
        "  \n",
        "  topics_path = \"/content/gdrive/MyDrive/ULETH/topics_final_BART.jsonl\"\n",
        "  topics = list(utils.read_jsonl(topics_path)) if os.path.isfile(topics_path) else []\n",
        "  start = len(topics)\n",
        "\n",
        "  os.environ['MALLET_HOME'] = 'mallet-2.0.8'\n",
        "  mallet_path = 'mallet-2.0.8/bin/mallet'\n",
        "  \n",
        "  for cluster in test_data[start:]:\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    d = {}\n",
        "    d[\"cluster_id\"] = cluster[\"id\"];\n",
        "    d[\"topics\"] = {}\n",
        "    document_list = cluster['articles'][:num_articles]\n",
        "    document_id = [a[\"id\"] for a in document_list  if a[\"text\"] != '']\n",
        "    document_text = [a[\"text\"] for a in document_list if a[\"text\"] != '']\n",
        "\n",
        "    doc_clean = preprocess_data(document_text,{})\n",
        "\n",
        "    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\n",
        "    ldamallet = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=num_topics, id2word=dictionary)\n",
        "    gensimmodel = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
        "\n",
        "    del ldamallet\n",
        "\n",
        "    document_topic_words = []\n",
        "    for index in range(len(document_id)):\n",
        "      document_topics = gensimmodel.get_document_topics(doc_term_matrix[index])\n",
        "      document_topics = sorted(document_topics, key=lambda x: x[1], reverse=True) \n",
        "\n",
        "      topic_words = []\n",
        "      for topic, prop in document_topics:\n",
        "        topic_words += [word[0] for word in gensimmodel.show_topic(topic, num_words)]\n",
        "\n",
        "      document_topic_words.append(topic_words);\n",
        "      del topic_words\n",
        "      del document_topics\n",
        "    id_words = zip(document_id, document_topic_words)\n",
        "\n",
        "    for id,words in id_words:\n",
        "      d[\"topics\"][id] = words\n",
        "\n",
        "    del id_words\n",
        "    del words\n",
        "    del document_topic_words\n",
        "    del gensimmodel\n",
        "    topics.append(d)\n",
        "    del d\n",
        "\n",
        "    utils.write_jsonl(topics, topics_path, override=True)\n",
        "\n",
        "  return topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV5LIwb-0PKo"
      },
      "source": [
        "# Testing\n",
        "num_topics = 5\n",
        "num_words = 2\n",
        "\n",
        "topics = LDA_topics(test_data, num_topics = num_topics, num_words = num_words, num_articles=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xieMRmVe3Zd"
      },
      "source": [
        "## MMR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6IwJjiQiRHW"
      },
      "source": [
        "### MMR Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycErnloDNbZm",
        "outputId": "cc91249a-0a96-4700-9067-126666ce7678"
      },
      "source": [
        "import gensim.downloader as api\n",
        "import nltk\n",
        "word2vec = api.load('word2vec-google-news-300')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def process_word_movers_distance(document, query_document, model = word2vec):\n",
        "    document = preprocess(document)\n",
        "    query_document = preprocess(query_document)\n",
        "    distance = model.wmdistance(document, query_document)\n",
        "    return -distance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IxzpRbA_ck8"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def process_tfidf_similarity(document, base_document):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # To make uniformed vectors, both documents need to be combined first.\n",
        "    document = [document]\n",
        "    document.insert(0, base_document)\n",
        "    embeddings = vectorizer.fit_transform(document)\n",
        "    tfidf_sims = cosine_similarity(embeddings[0:1], embeddings[1:]).flatten()\n",
        "    return tfidf_sims"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8VC0NImdQ_8"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "import string \n",
        "\n",
        "doc2vec = Doc2Vec.load(\"/content/gdrive/MyDrive/ULETH/doc2vec/doc2vec.bin\")\n",
        "\n",
        "def process_doc2vec_similarity(document, base_document, model = doc2vec):\n",
        "    # Both pretrained models are publicly available at public repo of jhlau.\n",
        "    # URL: https://github.com/jhlau/doc2vec\n",
        "\n",
        "    # Only handle words that appear in the doc2vec pretrained vectors.\n",
        "    # enwiki_dbow model contains 669549 vocabulary size.\n",
        "    tokens = preprocess(base_document)\n",
        "    tokens = list(filter(lambda x: x in model.wv.vocab.keys(), tokens))\n",
        "    base_vector = model.infer_vector(tokens)\n",
        "\n",
        "    tokens = preprocess(document)\n",
        "    tokens = list(filter(lambda x: x in model.wv.vocab.keys(), tokens))\n",
        "    vector = model.infer_vector(tokens)\n",
        "\n",
        "    scores = cosine_similarity([base_vector], [vector]).flatten()[0]\n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKb8Fu5Fujh-"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess(text):\n",
        "    # Steps:\n",
        "    # 1. lowercase\n",
        "    # 2. Lemmatize. (It does not stem. Try to preserve structure not to overwrap with potential acronym).\n",
        "    # 3. Remove stop words.\n",
        "    # 4. Remove punctuations.\n",
        "    # 5. Remove character with the length size of 1.\n",
        "\n",
        "    lowered = str.lower(text)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(lowered)\n",
        "\n",
        "    words = []\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            if w not in string.punctuation:\n",
        "                if len(w) > 1:\n",
        "                    lemmatized = lemmatizer.lemmatize(w)\n",
        "                    words.append(lemmatized)\n",
        "\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dURWNJmN5fYN"
      },
      "source": [
        "def is_all_stopwords(doc):\n",
        "    isallpunct = True\n",
        "    isalloneletter = True\n",
        "    isallstopwords = True\n",
        "    for x in doc.split():\n",
        "        if x not in string.punctuation:\n",
        "            isallpunct = False\n",
        "        if len(x) > 1:\n",
        "            isalloneletter = False\n",
        "        if x not in stop_words:\n",
        "            isallstopwords = False\n",
        "    return isallstopwords \\\n",
        "        or isallpunct \\\n",
        "        or isalloneletter \\\n",
        "        or len(doc) <= 3 \\\n",
        "        or doc.isspace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLhhp8s-fIMD"
      },
      "source": [
        "def compute_maximal_marginal_relevance(candidate_list, query, number_of_sentences=10, lambda_constant=1,\n",
        "                                       sim1=process_tfidf_similarity, sim2=process_tfidf_similarity, \n",
        "                                       sentences_to_return = None, mmr_percentage=0.1):\n",
        "    # candidate_list = tokenizer.tokenize(candidate_string)\n",
        "\n",
        "    candidate_list = [x for x in candidate_list if not is_all_stopwords(x)]\n",
        "\n",
        "    if candidate_list is None or len(candidate_list) == 0:\n",
        "        return ['']\n",
        "    \n",
        "    mmr_number = max(1, int(round(number_of_sentences * mmr_percentage)))\n",
        "    # Find best sentence to start, focused on relevance since there are\n",
        "    #   no selected sentences to compare to\n",
        "    mmr_number = mmr_number - 1 if sentences_to_return is None else mmr_number\n",
        "    if sentences_to_return is not None:\n",
        "      sentences_to_return = [x for x in sentences_to_return if not is_all_stopwords(x)]\n",
        "\n",
        "    else:  \n",
        "      initial_best_sentence = candidate_list[0]\n",
        "      prev = float(\"-inf\")\n",
        "      for sent in candidate_list:\n",
        "          similarity = sim1(sent, query)\n",
        "          if similarity != float(\"inf\") and similarity > prev:\n",
        "              initial_best_sentence = sent\n",
        "              prev = similarity\n",
        "      try:\n",
        "          candidate_list.remove(initial_best_sentence)\n",
        "      except ValueError:\n",
        "          pass  # do nothing\n",
        "\n",
        "      sentences_to_return = [initial_best_sentence]\n",
        "\n",
        "    for i in range(0, mmr_number):\n",
        "        previous_marginal_relevance = float(\"-inf\")\n",
        "        best_line = None\n",
        "        stand_in = None\n",
        "        for sent in candidate_list:\n",
        "            stand_in = sent\n",
        "            # Calculate the Marginal Relevance\n",
        "            left_side = lambda_constant * sim1(sent, query)\n",
        "            right_values = [float(\"-inf\")]\n",
        "            for selected_sentence in sentences_to_return:\n",
        "                right_values.append((1 - lambda_constant) * sim2(selected_sentence, sent))\n",
        "            right_side = max(right_values)\n",
        "            current_marginal_relevance = left_side - right_side\n",
        "            # Maximize Marginal Relevance\n",
        "            if current_marginal_relevance > previous_marginal_relevance:\n",
        "                previous_marginal_relevance = current_marginal_relevance\n",
        "                best_line = sent\n",
        "        # Update the returned sentences\n",
        "        if best_line is None:\n",
        "            best_line = stand_in\n",
        "        if len(candidate_list) > 0:\n",
        "            sentences_to_return += [best_line]\n",
        "            candidate_list.remove(best_line)\n",
        "\n",
        "    return sentences_to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esBscyqH03o3"
      },
      "source": [
        "### Load summaries and LDA topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYzJbEYE2VxW"
      },
      "source": [
        "import os.path\n",
        "path = \"/content/gdrive/MyDrive/ULETH/summaries_final.jsonl\"\n",
        "summaries = list(utils.read_jsonl(path)) if os.path.isfile(path) else []\n",
        "assert len(summaries) == 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SApNX7c32_3y"
      },
      "source": [
        "### MMR Concatenation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfPry64ix5BY"
      },
      "source": [
        "from experiments.evaluate import evaluate\n",
        "import experiments.sent_splitter as sent_splitter\n",
        "from tqdm import tqdm \n",
        "import gc\n",
        "import os.path \n",
        "\n",
        "sentSplitter = sent_splitter.SentenceSplitter()\n",
        "\n",
        "def generate_final_summaries(summaries, models, LDAtopics, test_data, lambda_constant, sim0, sim1, sim2, mmr_percentage, MMR_initial_sents, bestModel = \"Pegasus\"):\n",
        "  final_summaries_path = \"/content/gdrive/MyDrive/ULETH/finalsummaries_BART_v2.0.jsonl\"\n",
        "  final_summary_list = list(utils.read_jsonl(final_summaries_path)) if os.path.isfile(final_summaries_path) else []\n",
        "  start = len(final_summary_list)\n",
        "\n",
        "  for cluster in summaries[start:]:\n",
        "    c = cluster[\"cluster_id\"]\n",
        "    d = {}\n",
        "    d[\"cluster_id\"] = c\n",
        "\n",
        "    ground_summary = [x[\"summary\"] for x in test_data if x[\"id\"] == c][0]\n",
        "\n",
        "    candidate_sents = []\n",
        "    best_model_summary = []\n",
        "    for _,info in cluster[\"summaries\"].items():\n",
        "      for summ in info:\n",
        "        if summ[\"model\"] in goodmodels:\n",
        "          candidate_sents += sentSplitter.split_sents(summ[\"summary\"])\n",
        "        if summ[\"model\"] == bestModel:\n",
        "          # print(summ[\"rouge\"])\n",
        "          best_model_summary.append(summ[\"summary\"])\n",
        "        \n",
        "    # print(evaluate([\" \".join(best_model_summary)],[ground_summary]))\n",
        "\n",
        "    query_doc = []  \n",
        "    cluster_topics = [x[\"topics\"] for x in LDAtopics if x[\"cluster_id\"] == c][0]\n",
        "\n",
        "    for article_id, summ in cluster[\"summaries\"].items():\n",
        "      if len(summ) > 0:\n",
        "        query_doc += cluster_topics[article_id]\n",
        "    \n",
        "    query_doc = list(set(query_doc))\n",
        "    query_doc = \" \".join(query_doc)\n",
        "\n",
        "    score = []\n",
        "\n",
        "    for summ in best_model_summary:\n",
        "      score.append(str(sim0(summ, query_doc)))\n",
        "\n",
        "    best_model_summary = list(zip(score,best_model_summary))\n",
        "\n",
        "    best_model_summary = sorted(best_model_summary, key=lambda a: a[0], reverse=True)\n",
        "\n",
        "    best_model_summary = sentSplitter.split_sents(best_model_summary[0][1])\n",
        "    n = len(best_model_summary)\n",
        "\n",
        "    sentences_to_return = best_model_summary if MMR_initial_sents else None\n",
        "\n",
        "    try:\n",
        "      final_summary_sents = compute_maximal_marginal_relevance(candidate_sents, \n",
        "                                                             query_doc, \n",
        "                                                             number_of_sentences = n,\n",
        "                                                             lambda_constant= lambda_constant,\n",
        "                                                             sim1 = sim1, sim2 = sim2,\n",
        "                                                             sentences_to_return = sentences_to_return,\n",
        "                                                             mmr_percentage = mmr_percentage)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue\n",
        "    final_summary_sents = [s for s in final_summary_sents if s not in best_model_summary]\n",
        "    # print(len(final_summary_sents))\n",
        "    final_summary_sents = best_model_summary + final_summary_sents\n",
        "    final_summary = \" \".join(final_summary_sents)\n",
        "\n",
        "    d[\"rouge\"] = evaluate([final_summary],[ground_summary])\n",
        "    d[\"final_summary\"] = final_summary\n",
        "\n",
        "    final_summary_list.append(d)\n",
        "    del final_summary\n",
        "    del final_summary_sents\n",
        "    del best_model_summary\n",
        "    del candidate_sents\n",
        "    del query_doc\n",
        "\n",
        "    gc.collect()\n",
        "    \n",
        "    utils.write_jsonl(final_summary_list, final_summaries_path, override=True)\n",
        "  return final_summary_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mM8uIdrhVvo"
      },
      "source": [
        "from statistics import mean\n",
        "\n",
        "def score(final_summary_list):  \n",
        "  r1 = []\n",
        "  r2 = []\n",
        "  rl = []  \n",
        "\n",
        "  for c in final_summary_list:\n",
        "    rouge = c[\"rouge\"]\n",
        "    r1.append(rouge['rouge-1'])\n",
        "    r2.append(rouge['rouge-2'])\n",
        "    rl.append(rouge['rouge-l'])\n",
        "\n",
        "  r1p = [x['p'] for x in r1]\n",
        "  r1r = [x['r'] for x in r1]\n",
        "  r1f = [x['f'] for x in r1]\n",
        "  \n",
        "  r2p = [x['p'] for x in r2]\n",
        "  r2r = [x['r'] for x in r2]\n",
        "  r2f = [x['f'] for x in r2]\n",
        "  \n",
        "  rlp = [x['p'] for x in rl]\n",
        "  rlr = [x['r'] for x in rl]\n",
        "  rlf = [x['f'] for x in rl]\n",
        "\n",
        "  return (mean(r1p),mean(r1r),mean(r1f), mean(r2p),mean(r2r),mean(r2f), mean(rlp),mean(rlr),mean(rlf)) \n",
        "\n",
        "# print(\"r1 p: {:.3f} r: {:.3f} f: {:.3f} \\nr2 p: {:.3f} r: {:.3f} f: {:.3f} \\nrl p: {:.3f} r: {:.3f} f: {:.3f} \\n\".format(*score(final_summary_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JRBgh85T8VV"
      },
      "source": [
        "goodmodels = [\"LED\", \"BART\", \"T5\", \"Pegasus\", \"GPT2 EXT\", \"XLNet EXT\", \"BART EXT\"]\n",
        "\n",
        "lambda_constant =  0.9970352972330873\n",
        "mmr_percentage =  0.2979940927185918\n",
        "sim0 = process_doc2vec_similarity\n",
        "sim1 = process_word_movers_distance\n",
        "sim2 = process_tfidf_similarity\n",
        "MMR_initial_sents = True\n",
        "\n",
        "final_summary_list = generate_final_summaries(summaries, models = goodmodels, LDAtopics = topics,\n",
        "                                              test_data = test_data, lambda_constant = lambda_constant, \n",
        "                                              sim0 = sim0, sim1 = sim1, sim2 = sim2, \n",
        "                                              MMR_initial_sents = MMR_initial_sents, mmr_percentage = mmr_percentage,\n",
        "                                              bestModel = 'BART')\n",
        "\n",
        "score(final_summary_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0bPdXD32Yr5"
      },
      "source": [
        "## Hyperparameter optimisation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eEzSb3YJtE0"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyzOr9LxJrAl"
      },
      "source": [
        "import operator\n",
        "import numpy as np\n",
        "import optuna\n",
        "import random\n",
        "\n",
        "random.seed(10)\n",
        "goodmodels = [\"LED\", \"BART\", \"T5\", \"Pegasus\", \"GPT2 EXT\", \"XLNet EXT\", \"BART EXT\"]\n",
        "\n",
        "def objective(trial: optuna.Trial):\n",
        "\n",
        "  lambda_constant = trial.suggest_float('lambda_constant', 1e-5, 1.0, log=False)\n",
        "  num_topics = trial.suggest_int('num_topics', 1, 8)\n",
        "  num_words = trial.suggest_int('num_words', 1, 8)\n",
        "  mmr_percentage = trial.suggest_float('mmr_percentage', 0.0, 1.0, log=False)\n",
        "  sim1 = trial.suggest_categorical('sim1', ('process_tfidf_similarity', 'process_doc2vec_similarity', 'process_word_movers_distance'))\n",
        "  sim2 = trial.suggest_categorical('sim2', ('process_tfidf_similarity', 'process_doc2vec_similarity', 'process_word_movers_distance'))\n",
        "  sim0 = trial.suggest_categorical('sim0', ('process_tfidf_similarity', 'process_doc2vec_similarity', 'process_word_movers_distance'))\n",
        "  MMR_initial_sents = trial.suggest_categorical('MMR_initial_sents', (True,False))\n",
        "\n",
        "  if sim0 == 'process_tfidf_similarity':\n",
        "    sim0 = process_tfidf_similarity\n",
        "  elif sim0 == 'process_doc2vec_similarity':\n",
        "    sim0 = process_doc2vec_similarity\n",
        "  elif sim0 == 'process_word_movers_distance':\n",
        "    sim0 = process_word_movers_distance\n",
        "\n",
        "  if sim1 == 'process_tfidf_similarity':\n",
        "    sim1 = process_tfidf_similarity\n",
        "  elif sim1 == 'process_doc2vec_similarity':\n",
        "    sim1 = process_doc2vec_similarity\n",
        "  elif sim1 == 'process_word_movers_distance':\n",
        "    sim1 = process_word_movers_distance\n",
        "\n",
        "  if sim2 == 'process_tfidf_similarity':\n",
        "    sim2 = process_tfidf_similarity\n",
        "  elif sim2 == 'process_doc2vec_similarity':\n",
        "    sim2 = process_doc2vec_similarity\n",
        "  elif sim2 == 'process_word_movers_distance':\n",
        "    sim2 = process_word_movers_distance\n",
        "  \n",
        "  LDAtopics = LDA_topics(test_data = partial_test_data, num_topics = num_topics, num_words = num_words, num_articles=10)\n",
        "  summary_list = generate_final_summaries(summaries = summaries, models = goodmodels, LDAtopics = LDAtopics,\n",
        "                                              test_data = partial_test_data,lambda_constant = lambda_constant,\n",
        "                                              sim0 = sim0, sim1 = sim1, sim2 = sim2, \n",
        "                                              MMR_initial_sents = MMR_initial_sents, mmr_percentage = mmr_percentage,\n",
        "                                              bestModel = 'BART')\n",
        "  summary_score = score(summary_list)\n",
        "  \n",
        "  return summary_score[2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fej3wQJHvh9Z"
      },
      "source": [
        "summaries = list(utils.read_jsonl(\"/content/gdrive/MyDrive/ULETH/summaries_partial.jsonl\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba9Ppc9HLbNY"
      },
      "source": [
        "import pickle\n",
        "direction = \"maximize\"\n",
        "study = optuna.create_study(study_name=\"\" ,direction=direction)\n",
        "study.optimize(objective, n_trials = 100)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/ULETH/results_final.pkl',\"wb\") as result:\n",
        "  pickle.dump(study, result, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(study.best_params)\n",
        "print(study.best_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tIS-ciatbuT"
      },
      "source": [
        "import pickle\n",
        "import optuna\n",
        "\n",
        "with open('/content/gdrive/MyDrive/ULETH/results_final.pkl', 'rb') as result:\n",
        "    study = pickle.load(result)\n",
        "\n",
        "print(study.best_params)\n",
        "print(study.best_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kml6dQuQJWo2"
      },
      "source": [
        "fig = optuna.visualization.plot_param_importances(study)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}