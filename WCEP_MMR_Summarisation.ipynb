{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " WCEP MMR Summarisation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMtJCKRpE41X1gnYO2P4wKg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThisDavidAdams/MMR-summarization/blob/main/WCEP_MMR_Summarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8877kYHgFAza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4006e1ed-b2d8-454e-e5ea-5de40da56402"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5z_cB7BqW8E"
      },
      "source": [
        "## Clone WCEP Repository and install dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSbPyLEkqGvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f6a3bf-b8eb-41aa-8ce5-682a0b8a8bb9"
      },
      "source": [
        "!git clone https://github.com/complementizer/wcep-mds-dataset\n",
        "# Note: Kindly change line 4 in experiments/evaluate.py to 'import experiment.utils' from 'import utils'\n",
        "\n",
        "%cd wcep-mds-dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'wcep-mds-dataset'...\n",
            "remote: Enumerating objects: 125, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 125 (delta 65), reused 89 (delta 37), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (125/125), 1.09 MiB | 8.01 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "/content/wcep-mds-dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCyycE6WqeIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd715ba-affe-4751-c04a-189c8902762e"
      },
      "source": [
        "!pip install -r experiments/requirements.txt\n",
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/7e/74e707b66490d4eb05f702966ad0990881127acecf9d5cdcef3c95ec6c16/scikit_learn-0.23.1-cp37-cp37m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 4.9MB/s \n",
            "\u001b[?25hCollecting networkx==2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/8f/dd6a8e85946def36e4f2c69c84219af0fa5e832b018c970e92f2ad337e45/networkx-2.4-py3-none-any.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 36.9MB/s \n",
            "\u001b[?25hCollecting nltk==3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r experiments/requirements.txt (line 4)) (1.19.5)\n",
            "Collecting newsroom\n",
            "  Cloning git://github.com/clic-lab/newsroom.git to /tmp/pip-install-xejkux8j/newsroom\n",
            "  Running command git clone -q git://github.com/clic-lab/newsroom.git /tmp/pip-install-xejkux8j/newsroom\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/e8/c216b9b60cbba4642d3ca1bae7a53daa0c24426f662e0e3ce3dc7f6caeaa/threadpoolctl-2.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1->-r experiments/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1->-r experiments/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx==2.4->-r experiments/requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r experiments/requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r experiments/requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r experiments/requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from newsroom->-r experiments/requirements.txt (line 5)) (4.6.3)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from newsroom->-r experiments/requirements.txt (line 5)) (1.1.5)\n",
            "Collecting pyrouge>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.1MB/s \n",
            "\u001b[?25hCollecting readability-lxml>=0.6.2\n",
            "  Downloading https://files.pythonhosted.org/packages/39/a6/cfe22aaa19ac69b97d127043a76a5bbcb0ef24f3a0b22793c46608190caa/readability_lxml-0.8.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from newsroom->-r experiments/requirements.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: spacy>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from newsroom->-r experiments/requirements.txt (line 5)) (2.2.4)\n",
            "Collecting ujson>=1.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 40.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->newsroom->-r experiments/requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->newsroom->-r experiments/requirements.txt (line 5)) (2018.9)\n",
            "Collecting cssselect\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from readability-lxml>=0.6.2->newsroom->-r experiments/requirements.txt (line 5)) (4.2.6)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from readability-lxml>=0.6.2->newsroom->-r experiments/requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->newsroom->-r experiments/requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->newsroom->-r experiments/requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->newsroom->-r experiments/requirements.txt (line 5)) (2021.5.30)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (57.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (2.0.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->newsroom->-r experiments/requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (4.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.4->newsroom->-r experiments/requirements.txt (line 5)) (3.7.4.3)\n",
            "Building wheels for collected packages: nltk, newsroom, pyrouge\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp37-none-any.whl size=1434690 sha256=0d0f358f83ec1f5db49ce1aec2b758f7fd11c0af6249378e5db26a8b23ec61ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "  Building wheel for newsroom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for newsroom: filename=newsroom-0.1-cp37-none-any.whl size=281701 sha256=2e70c8954f80c08cb7bdf6f33a244a2525584f1174aa4c65126be48940c8af68\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-chuw0w7j/wheels/d5/c4/c7/23cd619d0c4202ef81a4509bfbaa6d90d89200c3ee15e518ba\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrouge: filename=pyrouge-0.1.3-cp37-none-any.whl size=191621 sha256=39daabd6855e12370f329fdf3e052edf54d1be677b7d3473001426efebbaa9b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
            "Successfully built nltk newsroom pyrouge\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: threadpoolctl, scikit-learn, networkx, nltk, pyrouge, cssselect, readability-lxml, ujson, newsroom\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: networkx 2.5.1\n",
            "    Uninstalling networkx-2.5.1:\n",
            "      Successfully uninstalled networkx-2.5.1\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed cssselect-1.1.0 networkx-2.4 newsroom-0.1 nltk-3.5 pyrouge-0.1.3 readability-lxml-0.8.1 scikit-learn-0.23.1 threadpoolctl-2.2.0 ujson-4.0.2\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zZ0mzY5qlNd"
      },
      "source": [
        "## Download the test dataset\n",
        "\n",
        "WCEP-100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mWFuB0RqVfS"
      },
      "source": [
        "!mkdir WCEP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqd0J5Yaq4Sy"
      },
      "source": [
        "import experiments.utils as utils\n",
        "\n",
        "test_data = list(utils.read_jsonl('/content/gdrive/MyDrive/MMRSumm/data/test_data.jsonl'))\n",
        "partial_test_data = test_data[:10]\n",
        "print(\"Number of clusters:\",len(test_data))\n",
        "print(test_data[0].keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPMbjpSnjrK1"
      },
      "source": [
        "summary_max = 0\n",
        "article_max = 0\n",
        "for c in test_data:\n",
        "  summary_max = max(summary_max,len(c['summary'].split(\" \")))\n",
        "\n",
        "  for a in c['articles']:\n",
        "    if article_max < len(a['text'].split(\" \")):\n",
        "      long_text = a[\"text\"]\n",
        "    article_max = max(article_max,len(a['text'].split(\" \")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR6bmVdKoejH"
      },
      "source": [
        "test_data = test_data[:500]\n",
        "assert len(test_data) == 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twaUMwN-igpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3cc3cde-9ef5-4030-fdd8-831fe98e54cf"
      },
      "source": [
        "from statistics import mean\n",
        "import experiments.sent_splitter as sent_splitter\n",
        "\n",
        "sentSplitter = sent_splitter.SentenceSplitter()\n",
        "\n",
        "article_word_count = [len(a['text'].split()) for c in test_data for a in c[\"articles\"]]\n",
        "summary_word_count = [len(c['summary'].split()) for c in test_data]\n",
        "summary_sent_length = [len(sentSplitter.split_sents(c[\"summary\"])) for c in test_data]\n",
        "\n",
        "print(\"max word count of articles:\",max(article_word_count))\n",
        "print(\"max word count of summary:\",max(summary_word_count))\n",
        "print(\"avg word count of articles:\", int(mean(article_word_count)))\n",
        "print(\"avg word count of summary:\", int(mean(summary_word_count)))\n",
        "print(\"max sent count of summary:\",max(summary_sent_length))\n",
        "print(\"avg sent count of summary:\",int(mean(summary_sent_length)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max word count of articles: 3096\n",
            "max word count of summary: 98\n",
            "avg word count of articles: 170\n",
            "avg word count of summary: 31\n",
            "max sent count of summary: 5\n",
            "avg sent count of summary: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ks1JlK15eOU3",
        "outputId": "a1c736d0-20d0-457d-894b-71c8ca098dca"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(1, 2, 1)  \n",
        "plt.hist(article_word_count, bins = 500)\n",
        "\n",
        "plt.subplot(1, 2, 2)  \n",
        "plt.hist(summary_word_count)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMthc1ikgLTL"
      },
      "source": [
        "max_summary_length = 150\n",
        "min_summary_length = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxHK-XQ8xYKM"
      },
      "source": [
        "## Importing the pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-HyLgj4g_Xd"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install spacy\n",
        "!pip install bert-extractive-summarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUtHoaZX07UL"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptT6ItiRoZzg",
        "outputId": "16f95370-d9cd-4431-d8bc-1f4de494b89c"
      },
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5-dKDHGCWlZ"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TuZpgzJDz-T"
      },
      "source": [
        "#### Abstractive Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FewxFm9gntW-"
      },
      "source": [
        "# PEGASUS\n",
        "\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "\n",
        "class Pegasus():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"Pegasus\"\n",
        "    self.model_name = \"google/pegasus-xsum\"\n",
        "    self.tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "    self.device = device\n",
        "    self.model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\").to(device)\n",
        "  \n",
        "  def summarise(self, text):\n",
        "    inputs = self.tokenizer([text], max_length=512, truncation = True, return_tensors='pt').to(self.device)\n",
        "    translated = self.model.generate(inputs['input_ids'],\n",
        "                                        min_length=min_summary_length,\n",
        "                                        max_length=max_summary_length,\n",
        "                                        temperature=0.6,\n",
        "                                        early_stopping=True,\n",
        "                                        top_k = 50,\n",
        "                                        no_repeat_ngram_size= 4,\n",
        "                                    )\n",
        "    \n",
        "    summary = self.tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "    del inputs\n",
        "    del translated\n",
        "\n",
        "    return summary[0]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48miFfzMFetk"
      },
      "source": [
        "# T5\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "\n",
        "class T5():\n",
        "  \n",
        "  def __init__(self,device):\n",
        "    self.name = \"T5\"\n",
        "    self.device = device\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained('t5-large').to(device)\n",
        "    self.tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
        "\n",
        "  def summarise(self, text):\n",
        "    text = \"summarize: \" + text\n",
        "    tokenized_text = self.tokenizer.encode(text, return_tensors=\"pt\", truncation = True).to(self.device)\n",
        "\n",
        "    summary_ids = self.model.generate(tokenized_text,\n",
        "                                        min_length=min_summary_length,\n",
        "                                        max_length=max_summary_length,\n",
        "                                        temperature=0.7,\n",
        "                                        early_stopping=True,\n",
        "                                        top_k = 50,\n",
        "                                        no_repeat_ngram_size= 4,\n",
        "                                      )\n",
        "    \n",
        "    summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    del text\n",
        "    del tokenized_text\n",
        "    del summary_ids\n",
        "    return summary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wth4pvBTRRqm"
      },
      "source": [
        "# GPT2\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "class GPT2():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"GPT2\"\n",
        "    self.device = device\n",
        "    self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(self.device)\n",
        "    self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "  def summarise(self,text):\n",
        "    input_ids = self.tokenizer.encode(text, return_tensors = 'pt', truncation=True, max_length = 1020 - max_summary_length)\n",
        "    tldr = self.tokenizer.encode(\" TL;DR:\", return_tensors = 'pt')\n",
        "    input_ids = torch.cat((input_ids,tldr),-1)\n",
        "    input_ids = input_ids.to(self.device)\n",
        "    beam_output = self.model.generate(input_ids, \n",
        "                                      min_length = len(input_ids[0]) + min_summary_length,\n",
        "                                      max_length=len(input_ids[0]) + max_summary_length,\n",
        "                                      temperature=0.6,\n",
        "                                      early_stopping=True,\n",
        "                                      top_k = 50,\n",
        "                                      no_repeat_ngram_size= 4,\n",
        "                                    )\n",
        "    output = self.tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "    \n",
        "    summary = output.split(\"TL;DR:\")[-1]\n",
        "    del input_ids\n",
        "    del tldr\n",
        "    del beam_output\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9tlGNLIvPya"
      },
      "source": [
        "# XLNet\n",
        "\n",
        "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
        "\n",
        "class XLNet():\n",
        "  \n",
        "  def __init__(self,device):\n",
        "    self.name = \"XLNet\"\n",
        "    self.device = device\n",
        "    self.tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
        "    self.model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased').to(device)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    input_ids = self.tokenizer.encode(text, return_tensors = 'pt', truncation=True, max_length = 1020 - max_summary_length)\n",
        "    tldr = self.tokenizer.encode(\" TL;DR:\", return_tensors = 'pt')\n",
        "    input_ids = torch.cat((input_ids,tldr),-1)\n",
        "    input_ids = input_ids.to(self.device)\n",
        "    beam_output = self.model.generate(input_ids,\n",
        "                                      min_length=len(input_ids[0]) + min_summary_length,\n",
        "                                      max_length=len(input_ids[0]) + max_summary_length,\n",
        "                                      temperature=0.6,\n",
        "                                      early_stopping=True,\n",
        "                                      top_k = 50,\n",
        "                                      no_repeat_ngram_size= 4,\n",
        "                                    )\n",
        "    \n",
        "    output = self.tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "    output = output.split(\"TL;DR:\")[-1]\n",
        "    del input_ids\n",
        "    del tldr\n",
        "    del beam_output\n",
        "    \n",
        "    return output\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdBfYdER-B7V"
      },
      "source": [
        "# ProphetNet\n",
        "\n",
        "from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration\n",
        "\n",
        "class ProphetNet():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"ProphetNet\"\n",
        "    self.device = device\n",
        "    self.tokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased')\n",
        "    self.model = ProphetNetForConditionalGeneration.from_pretrained('microsoft/prophetnet-large-uncased').to(self.device)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    input_ids = self.tokenizer(text, return_tensors=\"pt\", truncation = True).input_ids\n",
        "    decoder_input_ids = self.tokenizer(\"To summarise\", return_tensors=\"pt\").input_ids  \n",
        "    input_ids = input_ids.to(self.device)\n",
        "    decoder_input_ids = decoder_input_ids.to(self.device)\n",
        "\n",
        "    beam_output = self.model.generate(input_ids, \n",
        "                                      decoder_input_ids = decoder_input_ids,\n",
        "                                      min_length = min_summary_length,\n",
        "                                      max_length = max_summary_length,\n",
        "                                      temperature=0.7,\n",
        "                                      early_stopping=True,\n",
        "                                      top_k = 50,\n",
        "                                      no_repeat_ngram_size= 4,\n",
        "                                )\n",
        "    \n",
        "    output = self.tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "    del input_ids\n",
        "    del decoder_input_ids\n",
        "    del beam_output\n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMindhh-hsvu"
      },
      "source": [
        "# BART\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "\n",
        "class BART():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"BART\"\n",
        "    self.device = device\n",
        "    self.model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
        "    self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "  def summarise(self, text, MDS = False):\n",
        "    inputs = self.tokenizer([text], max_length=1024, truncation = True, return_tensors='pt').to(self.device)\n",
        "    summary_ids = self.model.generate(inputs['input_ids'], \n",
        "                                      min_length = min_summary_length,\n",
        "                                      max_length = max_summary_length,\n",
        "                                      temperature=0.7,\n",
        "                                      early_stopping=True,\n",
        "                                      top_k = 50,\n",
        "                                      no_repeat_ngram_size= 4,\n",
        "                                    )\n",
        "    \n",
        "    summary = ' '.join([(self.tokenizer).decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
        "    del inputs\n",
        "    del summary_ids\n",
        "\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lon9v6ReoddZ"
      },
      "source": [
        "# LED\n",
        "\n",
        "from transformers import LEDTokenizer, LEDForConditionalGeneration, LEDConfig\n",
        "\n",
        "class LED():\n",
        "\n",
        "  def __init__(self,device):\n",
        "    self.name = \"LED\"\n",
        "    self.device = device\n",
        "    self.model = LEDForConditionalGeneration.from_pretrained('allenai/led-base-16384').to(self.device)\n",
        "    self.tokenizer = LEDTokenizer.from_pretrained('allenai/led-base-16384')\n",
        "\n",
        "  def summarise(self, text, MDS = False):\n",
        "    inputs = self.tokenizer([text], max_length=1024, truncation=True, return_tensors='pt').to(self.device)\n",
        "    summary_ids = self.model.generate(inputs['input_ids'], \n",
        "                                      min_length = min_summary_length,\n",
        "                                      max_length = max_summary_length,\n",
        "                                      temperature=0.7,\n",
        "                                      early_stopping=True,\n",
        "                                      top_k = 50,\n",
        "                                      no_repeat_ngram_size= 4,\n",
        "                                    )\n",
        "    summary = ' '.join([(self.tokenizer).decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
        "    del inputs\n",
        "    del summary_ids\n",
        "\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsp2KkfmD59V"
      },
      "source": [
        "#### Extractive Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z28SohN-zGU"
      },
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
        "from summarizer import Summarizer\n",
        "# GPT2 extractive\n",
        "\n",
        "class GPT2_ext():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.name = \"GPT2 EXT\"\n",
        "    model_name = \"gpt2\"\n",
        "    custom_config = AutoConfig.from_pretrained(model_name)\n",
        "    custom_config.output_hidden_states=True\n",
        "    custom_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    custom_model = AutoModel.from_pretrained(model_name, config=custom_config)\n",
        "    self.model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    return self.model(text,ratio = 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU3o9cSD-0NH"
      },
      "source": [
        "# XLNet extractive\n",
        "\n",
        "class XLNet_ext():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.name = \"XLNet EXT\"\n",
        "    model_name = \"xlnet-large-cased\"\n",
        "    custom_config = AutoConfig.from_pretrained(model_name)\n",
        "    custom_config.output_hidden_states=True\n",
        "    custom_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    custom_model = AutoModel.from_pretrained(model_name, config=custom_config)\n",
        "    self.model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    return self.model(text,ratio = 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYAji204_iMu"
      },
      "source": [
        "# BART extractive\n",
        "\n",
        "class BART_ext():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.name = \"BART EXT\"\n",
        "    model_name = \"facebook/bart-large-cnn\"\n",
        "    custom_config = AutoConfig.from_pretrained(model_name)\n",
        "    custom_config.output_hidden_states=True\n",
        "    custom_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    custom_model = AutoModel.from_pretrained(model_name, config=custom_config)\n",
        "    self.model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
        "\n",
        "  def summarise(self,text):\n",
        "    return self.model(text,ratio = 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoinV-r3XqxE"
      },
      "source": [
        "## Generate Summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kceh3_OG_4-m"
      },
      "source": [
        "%%time \n",
        "\n",
        "prophetnet = ProphetNet(device)\n",
        "gpt2 = GPT2(device)\n",
        "xlnet = XLNet(device)\n",
        "t5 = T5(device)\n",
        "led = LED(device)\n",
        "bart = BART(device)\n",
        "pegasus = Pegasus(device)\n",
        "\n",
        "gpt2_ext = GPT2_ext()\n",
        "xlnet_ext = XLNet_ext()\n",
        "# t5_ext = T5_ext()\n",
        "# led_ext = LED_ext()\n",
        "bart_ext = BART_ext()\n",
        "# pegasus_ext = Pegasus_ext()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-pu1tps__RY"
      },
      "source": [
        "models = {\n",
        "      \"ProphetNet\" : prophetnet,\n",
        "      \"GPT2\" : gpt2,\n",
        "      \"T5\" : t5,\n",
        "      \"XLNet\" : xlnet,\n",
        "      \"LED\" : led,\n",
        "      \"BART\" : bart,\n",
        "      \"Pegasus\" : pegasus,\n",
        "      \"GPT2 EXT\": gpt2_ext,\n",
        "      \"XLNet EXT\": xlnet_ext,\n",
        "      \"BART EXT\": bart_ext,\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGb_PK6k1Y5M"
      },
      "source": [
        "from experiments.evaluate import evaluate\n",
        "from tqdm import tqdm \n",
        "import os.path\n",
        "import gc\n",
        "import re\n",
        "from datetime import datetime\n",
        "# Generating summaries\n",
        "\n",
        "def generateBaselineSummaries(test_data, num_articles, models = models):\n",
        "  path = \"/content/gdrive/MyDrive/MMRSumm/outputs/summaries.jsonl\"\n",
        "  summaries = list(utils.read_jsonl(path)) if os.path.isfile(path) else []\n",
        "  start = len(summaries)\n",
        "  # summary keys : summary, type (MDS/SDS), index (None/article_index), model, clusterid, Rouge_score, MMR_reduced\n",
        "\n",
        "  print(\"Continuing from cluster {}\".format(start+1))\n",
        "\n",
        "  for cluster in test_data[start:]:\n",
        "    \n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    c = {} \n",
        "    c[\"cluster_id\"] = cluster[\"id\"]\n",
        "    c[\"summaries\"] = {}\n",
        "\n",
        "    for article in cluster['articles'][:num_articles]:\n",
        "      c[\"summaries\"][article[\"id\"]] = []\n",
        "\n",
        "    for _,model in models.items():\n",
        "      for article in cluster['articles'][:num_articles]:\n",
        "\n",
        "        #Clean up CUDA\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        d = {}\n",
        "        text = article[\"text\"]\n",
        "        if text == '':\n",
        "          continue\n",
        "\n",
        "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        summary = model.summarise(text)\n",
        "\n",
        "        d[\"type\"] = \"SDS\"\n",
        "        d[\"model\"] = model.name\n",
        "\n",
        "        d[\"summary\"] = summary\n",
        "\n",
        "        rouge = evaluate([cluster[\"summary\"]],[summary])\n",
        "        d[\"rouge\"] = rouge\n",
        "        c[\"summaries\"][article[\"id\"]].append(d)\n",
        "\n",
        "        del summary\n",
        "        del d\n",
        "        del text\n",
        "        del rouge\n",
        "\n",
        "    summaries.append(c) \n",
        "    del c\n",
        "\n",
        "    utils.write_jsonl(summaries, path=path, override = True)\n",
        "  return summaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWeHl78J-6Gw"
      },
      "source": [
        "summaries = generateBaselineSummaries(test_data, num_articles = 10, models = models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCETJ0OsXzY1"
      },
      "source": [
        "### Baseline models score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Ou395S5nto"
      },
      "source": [
        "import os.path\n",
        "path = \"/content/gdrive/MyDrive/MMRSumm/outputs/summaries.jsonl\"\n",
        "summaries = list(utils.read_jsonl(path)) if os.path.isfile(path) else []\n",
        "assert len(summaries) == 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7y6mSig8yio"
      },
      "source": [
        "summary_results = {}\n",
        "\n",
        "model_names = set()\n",
        "\n",
        "for d in summaries:\n",
        "  for _, summ in d[\"summaries\"].items():\n",
        "    for s in summ:\n",
        "      model_names.add(s[\"model\"])\n",
        "\n",
        "model_names = list(model_names)\n",
        "\n",
        "for m in model_names:\n",
        "  summary_results[m] = []\n",
        "\n",
        "for d in summaries:\n",
        "  for _, summ in d[\"summaries\"].items():\n",
        "    for s in summ:\n",
        "      summary_results[s[\"model\"]].append(s[\"rouge\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHFCd9D4D_GJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df47942-6115-42ba-fb80-d65af78f34dc"
      },
      "source": [
        "from statistics import mean\n",
        "for model,scores in summary_results.items():\n",
        "  r1 = []\n",
        "  r2 = []\n",
        "  rl = []\n",
        "\n",
        "  for s in scores:\n",
        "    r1.append(s[\"rouge-1\"])\n",
        "    r2.append(s[\"rouge-2\"])\n",
        "    rl.append(s[\"rouge-l\"])\n",
        "  \n",
        "  r1p = mean([x['p'] for x in r1]) *100\n",
        "  r1r = mean([x['r'] for x in r1]) *100\n",
        "  r1f = mean([x['f'] for x in r1]) *100\n",
        "  \n",
        "  r2p = mean([x['p'] for x in r2]) *100\n",
        "  r2r = mean([x['r'] for x in r2]) *100\n",
        "  r2f = mean([x['f'] for x in r2]) *100\n",
        "  \n",
        "  rlp = mean([x['p'] for x in rl]) *100\n",
        "  rlr = mean([x['r'] for x in rl]) *100\n",
        "  rlf = mean([x['f'] for x in rl]) *100\n",
        "  \n",
        "  print(model)\n",
        "  print(\"r1 p: {:.3f} r: {:.3f} f: {:.3f} \\nr2 p: {:.3f} r: {:.3f} f: {:.3f} \\nrl p: {:.3f} r: {:.3f} f: {:.3f} \\n\".format(r1p,r1r,r1f,r2p,r2r,r2f,rlp,rlr,rlf))\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BART\n",
            "r1 p: 37.421 r: 24.323 f: 28.117 \n",
            "r2 p: 11.975 r: 7.578 f: 8.842 \n",
            "rl p: 26.674 r: 16.894 f: 19.712 \n",
            "\n",
            "T5\n",
            "r1 p: 29.760 r: 22.125 f: 24.141 \n",
            "r2 p: 7.422 r: 5.459 f: 5.980 \n",
            "rl p: 21.171 r: 15.398 f: 16.953 \n",
            "\n",
            "ProphetNet\n",
            "r1 p: 21.724 r: 11.911 f: 14.118 \n",
            "r2 p: 3.841 r: 1.890 f: 2.356 \n",
            "rl p: 16.463 r: 8.904 f: 10.562 \n",
            "\n",
            "XLNet EXT\n",
            "r1 p: 42.815 r: 23.835 f: 27.411 \n",
            "r2 p: 14.965 r: 8.222 f: 9.485 \n",
            "rl p: 30.529 r: 17.005 f: 19.484 \n",
            "\n",
            "GPT2\n",
            "r1 p: 35.058 r: 11.191 f: 15.890 \n",
            "r2 p: 7.243 r: 2.114 f: 3.079 \n",
            "rl p: 25.205 r: 7.844 f: 11.176 \n",
            "\n",
            "XLNet\n",
            "r1 p: 24.510 r: 11.795 f: 12.888 \n",
            "r2 p: 5.468 r: 2.157 f: 2.702 \n",
            "rl p: 18.066 r: 9.082 f: 9.475 \n",
            "\n",
            "BART EXT\n",
            "r1 p: 42.976 r: 23.671 f: 27.166 \n",
            "r2 p: 15.052 r: 8.170 f: 9.394 \n",
            "rl p: 30.754 r: 16.971 f: 19.386 \n",
            "\n",
            "GPT2 EXT\n",
            "r1 p: 43.193 r: 23.620 f: 27.325 \n",
            "r2 p: 15.076 r: 8.126 f: 9.416 \n",
            "rl p: 30.991 r: 16.923 f: 19.516 \n",
            "\n",
            "LED\n",
            "r1 p: 51.273 r: 17.473 f: 24.382 \n",
            "r2 p: 18.565 r: 6.066 f: 8.546 \n",
            "rl p: 35.692 r: 11.981 f: 16.720 \n",
            "\n",
            "Pegasus\n",
            "r1 p: 36.066 r: 23.882 f: 27.330 \n",
            "r2 p: 11.367 r: 7.188 f: 8.378 \n",
            "rl p: 25.719 r: 16.612 f: 19.190 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8HkVc91jb9l"
      },
      "source": [
        "## Setting up LDAMallet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wy8zn2Cz575"
      },
      "source": [
        "### Load summaires"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMVO9W_dzt9f"
      },
      "source": [
        "import os.path\n",
        "path = \"/content/gdrive/MyDrive/MMRSumm/outputs/summaries.jsonl\"\n",
        "summaries = list(utils.read_jsonl(path)) if os.path.isfile(path) else []\n",
        "assert len(summaries) == 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSYKu5C50EOg"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PcpmeuIjfO-"
      },
      "source": [
        "!pip install gensim==3.8.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2jovFPFjggG"
      },
      "source": [
        "# install JAVA\n",
        "import os       \n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujgB_I-Cjn70"
      },
      "source": [
        "# Install Mallet\n",
        "# !rm -rf /content/wcep-mds-dataset/mallet-2.0.8\n",
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoJnf4b3ojD1"
      },
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.wrappers import LdaMallet\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim import similarities\n",
        "\n",
        "import os.path\n",
        "import re\n",
        "import glob\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_Q_47OpomG6"
      },
      "source": [
        "def preprocess_data(doc_set, extra_stopwords = {}):\n",
        "    doc_set = [re.sub('\\s+', ' ', doc) for doc in doc_set]\n",
        "    doc_set = [doc.replace('\\n', ' ') for doc in doc_set]\n",
        "    doc_set = [doc.replace(\"'\", \"\") for doc in doc_set]\n",
        "    doc_set = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', doc, flags=re.MULTILINE) for doc in doc_set]\n",
        "\n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create English stop words list\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    # add any extra stopwords\n",
        "    if (len(extra_stopwords) > 0):\n",
        "        en_stop = en_stop.union(extra_stopwords)\n",
        "    \n",
        "    # list for tokenized documents in loop\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "    for i in doc_set:\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        # add tokens to list\n",
        "        texts.append(stopped_tokens)\n",
        "    return texts\n",
        "\n",
        "def prepare_corpus(doc_clean):\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # generate LDA model\n",
        "    return dictionary,doc_term_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEJ5TFE7w-31"
      },
      "source": [
        "def remove_stopwords(texts):\n",
        "  return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts, bigram_mod):\n",
        "  return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts, bigram_mod):\n",
        "  return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(docs, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "  \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "  texts_out = []\n",
        "  for sent in texts:\n",
        "      doc = nlp(\" \".join(sent))\n",
        "      texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "  return texts_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InBUAGcz0H8R"
      },
      "source": [
        "### Generate LDA topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl0_VFARqEfY"
      },
      "source": [
        "import gc\n",
        "from tqdm import tqdm\n",
        "import os.path\n",
        "\n",
        "def LDA_topics(test_data, num_topics, num_words, num_articles = 10):\n",
        "  \n",
        "  topics_path = \"/content/gdrive/MyDrive/MMRSumm/outputs/LDA_Topics.jsonl\"\n",
        "  topics = list(utils.read_jsonl(topics_path)) if os.path.isfile(topics_path) else []\n",
        "  start = len(topics)\n",
        "\n",
        "  os.environ['MALLET_HOME'] = 'mallet-2.0.8'\n",
        "  mallet_path = 'mallet-2.0.8/bin/mallet'\n",
        "  \n",
        "  print(\"Continuing from cluster {}\".format(start+1))\n",
        "  for cluster in test_data[start:]:\n",
        "    gc.collect()\n",
        "\n",
        "    d = {}\n",
        "    d[\"cluster_id\"] = cluster[\"id\"];\n",
        "    d[\"topics\"] = {}\n",
        "    document_list = cluster['articles'][:num_articles]\n",
        "    document_id = [a[\"id\"] for a in document_list  if a[\"text\"] != '']\n",
        "    document_text = [a[\"text\"] for a in document_list if a[\"text\"] != '']\n",
        "\n",
        "    doc_clean = preprocess_data(document_text,{})\n",
        "\n",
        "    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\n",
        "    ldamallet = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=num_topics, id2word=dictionary)\n",
        "    gensimmodel = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
        "\n",
        "    del ldamallet\n",
        "\n",
        "    document_topic_words = []\n",
        "    for index in range(len(document_id)):\n",
        "      document_topics = gensimmodel.get_document_topics(doc_term_matrix[index])\n",
        "      document_topics = sorted(document_topics, key=lambda x: x[1], reverse=True) \n",
        "\n",
        "      topic_words = []\n",
        "      for topic, prop in document_topics:\n",
        "        topic_words += [word[0] for word in gensimmodel.show_topic(topic, num_words)]\n",
        "\n",
        "      document_topic_words.append(topic_words);\n",
        "      del topic_words\n",
        "      del document_topics\n",
        "    id_words = zip(document_id, document_topic_words)\n",
        "\n",
        "    for id,words in id_words:\n",
        "      d[\"topics\"][id] = words\n",
        "\n",
        "    del id_words\n",
        "    del words\n",
        "    del document_topic_words\n",
        "    del gensimmodel\n",
        "    topics.append(d)\n",
        "    del d\n",
        "\n",
        "    utils.write_jsonl(topics, topics_path, override=True)\n",
        "\n",
        "  return topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV5LIwb-0PKo"
      },
      "source": [
        "# Testing\n",
        "num_topics = 5\n",
        "num_words = 2\n",
        "\n",
        "topics = LDA_topics(test_data, num_topics = num_topics, num_words = num_words, num_articles=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xieMRmVe3Zd"
      },
      "source": [
        "## MMR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6IwJjiQiRHW"
      },
      "source": [
        "### MMR Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycErnloDNbZm"
      },
      "source": [
        "import gensim.downloader as api\n",
        "import nltk\n",
        "word2vec = api.load('word2vec-google-news-300')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def process_word_movers_distance(document, query_document, model = word2vec):\n",
        "    document = preprocess(document)\n",
        "    query_document = preprocess(query_document)\n",
        "    distance = model.wmdistance(document, query_document)\n",
        "    return -distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IxzpRbA_ck8"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def process_tfidf_similarity(document, base_document):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # To make uniformed vectors, both documents need to be combined first.\n",
        "    document = [document]\n",
        "    document.insert(0, base_document)\n",
        "    embeddings = vectorizer.fit_transform(document)\n",
        "    tfidf_sims = cosine_similarity(embeddings[0:1], embeddings[1:]).flatten()\n",
        "    return tfidf_sims"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8VC0NImdQ_8"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "import string \n",
        "\n",
        "doc2vec = Doc2Vec.load(\"/content/gdrive/MyDrive/MMRSumm/models/doc2vec/doc2vec.bin\")\n",
        "\n",
        "def process_doc2vec_similarity(document, base_document, model = doc2vec):\n",
        "    # Both pretrained models are publicly available at public repo of jhlau.\n",
        "    # URL: https://github.com/jhlau/doc2vec\n",
        "\n",
        "    # Only handle words that appear in the doc2vec pretrained vectors.\n",
        "    # enwiki_dbow model contains 669549 vocabulary size.\n",
        "    tokens = preprocess(base_document)\n",
        "    tokens = list(filter(lambda x: x in model.wv.vocab.keys(), tokens))\n",
        "    base_vector = model.infer_vector(tokens)\n",
        "\n",
        "    tokens = preprocess(document)\n",
        "    tokens = list(filter(lambda x: x in model.wv.vocab.keys(), tokens))\n",
        "    vector = model.infer_vector(tokens)\n",
        "\n",
        "    scores = cosine_similarity([base_vector], [vector]).flatten()[0]\n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKb8Fu5Fujh-"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess(text):\n",
        "    # Steps:\n",
        "    # 1. lowercase\n",
        "    # 2. Lemmatize. (It does not stem. Try to preserve structure not to overwrap with potential acronym).\n",
        "    # 3. Remove stop words.\n",
        "    # 4. Remove punctuations.\n",
        "    # 5. Remove character with the length size of 1.\n",
        "\n",
        "    lowered = str.lower(text)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(lowered)\n",
        "\n",
        "    words = []\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            if w not in string.punctuation:\n",
        "                if len(w) > 1:\n",
        "                    lemmatized = lemmatizer.lemmatize(w)\n",
        "                    words.append(lemmatized)\n",
        "\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dURWNJmN5fYN"
      },
      "source": [
        "def is_all_stopwords(doc):\n",
        "    isallpunct = True\n",
        "    isalloneletter = True\n",
        "    isallstopwords = True\n",
        "    for x in doc.split():\n",
        "        if x not in string.punctuation:\n",
        "            isallpunct = False\n",
        "        if len(x) > 1:\n",
        "            isalloneletter = False\n",
        "        if x not in stop_words:\n",
        "            isallstopwords = False\n",
        "    return isallstopwords \\\n",
        "        or isallpunct \\\n",
        "        or isalloneletter \\\n",
        "        or len(doc) <= 3 \\\n",
        "        or doc.isspace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLhhp8s-fIMD"
      },
      "source": [
        "def compute_maximal_marginal_relevance(candidate_list, query, number_of_sentences=10, lambda_constant=1,\n",
        "                                       sim1=process_tfidf_similarity, sim2=process_tfidf_similarity, \n",
        "                                       sentences_to_return = None, mmr_percentage=0.1):\n",
        "    # candidate_list = tokenizer.tokenize(candidate_string)\n",
        "\n",
        "    candidate_list = [x for x in candidate_list if not is_all_stopwords(x)]\n",
        "\n",
        "    if candidate_list is None or len(candidate_list) == 0:\n",
        "        return ['']\n",
        "    \n",
        "    mmr_number = max(1, int(round(number_of_sentences * mmr_percentage)))\n",
        "    # Find best sentence to start, focused on relevance since there are\n",
        "    #   no selected sentences to compare to\n",
        "    mmr_number = mmr_number - 1 if sentences_to_return is None else mmr_number\n",
        "    if sentences_to_return is not None:\n",
        "      sentences_to_return = [x for x in sentences_to_return if not is_all_stopwords(x)]\n",
        "\n",
        "    else:  \n",
        "      initial_best_sentence = candidate_list[0]\n",
        "      prev = float(\"-inf\")\n",
        "      for sent in candidate_list:\n",
        "          similarity = sim1(sent, query)\n",
        "          if similarity != float(\"inf\") and similarity > prev:\n",
        "              initial_best_sentence = sent\n",
        "              prev = similarity\n",
        "      try:\n",
        "          candidate_list.remove(initial_best_sentence)\n",
        "      except ValueError:\n",
        "          pass  # do nothing\n",
        "\n",
        "      sentences_to_return = [initial_best_sentence]\n",
        "\n",
        "    for i in range(0, mmr_number):\n",
        "        previous_marginal_relevance = float(\"-inf\")\n",
        "        best_line = None\n",
        "        stand_in = None\n",
        "        for sent in candidate_list:\n",
        "            stand_in = sent\n",
        "            # Calculate the Marginal Relevance\n",
        "            left_side = lambda_constant * sim1(sent, query)\n",
        "            right_values = [float(\"-inf\")]\n",
        "            for selected_sentence in sentences_to_return:\n",
        "                right_values.append((1 - lambda_constant) * sim2(selected_sentence, sent))\n",
        "            right_side = max(right_values)\n",
        "            current_marginal_relevance = left_side - right_side\n",
        "            # Maximize Marginal Relevance\n",
        "            if current_marginal_relevance > previous_marginal_relevance:\n",
        "                previous_marginal_relevance = current_marginal_relevance\n",
        "                best_line = sent\n",
        "        # Update the returned sentences\n",
        "        if best_line is None:\n",
        "            best_line = stand_in\n",
        "        if len(candidate_list) > 0:\n",
        "            sentences_to_return += [best_line]\n",
        "            candidate_list.remove(best_line)\n",
        "\n",
        "    return sentences_to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esBscyqH03o3"
      },
      "source": [
        "### Load summaries and LDA topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYzJbEYE2VxW"
      },
      "source": [
        "import os.path\n",
        "path = \"/content/gdrive/MyDrive/MMRSumm/outputs/summaries.jsonl\"\n",
        "summaries = list(utils.read_jsonl(path)) if os.path.isfile(path) else []\n",
        "\n",
        "path = \"/content/gdrive/MyDrive/MMRSumm/outputs/LDA_Topics.jsonl\"\n",
        "topics = list(utils.read_jsonl(path)) if os.path.isfile(path) else []\n",
        "\n",
        "assert len(summaries) == 500\n",
        "assert len(topics) == 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SApNX7c32_3y"
      },
      "source": [
        "### MMR Concatenation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfPry64ix5BY"
      },
      "source": [
        "from experiments.evaluate import evaluate\n",
        "import experiments.sent_splitter as sent_splitter\n",
        "from tqdm import tqdm \n",
        "import gc\n",
        "import os.path \n",
        "\n",
        "sentSplitter = sent_splitter.SentenceSplitter()\n",
        "\n",
        "def generate_final_summaries(summaries, models, LDAtopics, test_data, lambda_constant, sim0, sim1, sim2, mmr_percentage, MMR_initial_sents, bestModel = \"Pegasus\"):\n",
        "  final_summaries_path = \"/content/gdrive/MyDrive/MMRSumm/outputs/finalsummaries_MMR.jsonl\"\n",
        "  final_summary_list = list(utils.read_jsonl(final_summaries_path)) if os.path.isfile(final_summaries_path) else []\n",
        "  start = len(final_summary_list)\n",
        "\n",
        "  for cluster in summaries[start:]:\n",
        "    c = cluster[\"cluster_id\"]\n",
        "    d = {}\n",
        "    d[\"cluster_id\"] = c\n",
        "\n",
        "    ground_summary = [x[\"summary\"] for x in test_data if x[\"id\"] == c][0]\n",
        "\n",
        "    candidate_sents = []\n",
        "    best_model_summary = []\n",
        "    for _,info in cluster[\"summaries\"].items():\n",
        "      for summ in info:\n",
        "        if summ[\"model\"] in goodmodels:\n",
        "          candidate_sents += sentSplitter.split_sents(summ[\"summary\"])\n",
        "        if summ[\"model\"] == bestModel:\n",
        "          # print(summ[\"rouge\"])\n",
        "          best_model_summary.append(summ[\"summary\"])\n",
        "        \n",
        "    # print(evaluate([\" \".join(best_model_summary)],[ground_summary]))\n",
        "\n",
        "    query_doc = []  \n",
        "    cluster_topics = [x[\"topics\"] for x in LDAtopics if x[\"cluster_id\"] == c][0]\n",
        "\n",
        "    for article_id, summ in cluster[\"summaries\"].items():\n",
        "      if len(summ) > 0:\n",
        "        query_doc += cluster_topics[article_id]\n",
        "    \n",
        "    query_doc = list(set(query_doc))\n",
        "    query_doc = \" \".join(query_doc)\n",
        "\n",
        "    score = []\n",
        "\n",
        "    for summ in best_model_summary:\n",
        "      score.append(str(sim0(summ, query_doc)))\n",
        "\n",
        "    best_model_summary = list(zip(score,best_model_summary))\n",
        "\n",
        "    best_model_summary = sorted(best_model_summary, key=lambda a: a[0], reverse=True)\n",
        "\n",
        "    best_model_summary = sentSplitter.split_sents(best_model_summary[0][1])\n",
        "    n = len(best_model_summary)\n",
        "\n",
        "    sentences_to_return = best_model_summary if MMR_initial_sents else None\n",
        "\n",
        "    try:\n",
        "      final_summary_sents = compute_maximal_marginal_relevance(candidate_sents, \n",
        "                                                             query_doc, \n",
        "                                                             number_of_sentences = n,\n",
        "                                                             lambda_constant= lambda_constant,\n",
        "                                                             sim1 = sim1, sim2 = sim2,\n",
        "                                                             sentences_to_return = sentences_to_return,\n",
        "                                                             mmr_percentage = mmr_percentage)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue\n",
        "    final_summary_sents = [s for s in final_summary_sents if s not in best_model_summary]\n",
        "    # print(len(final_summary_sents))\n",
        "    final_summary_sents = best_model_summary + final_summary_sents\n",
        "    final_summary = \" \".join(final_summary_sents)\n",
        "\n",
        "    d[\"rouge\"] = evaluate([ground_summary],[final_summary])\n",
        "    d[\"final_summary\"] = final_summary\n",
        "\n",
        "    final_summary_list.append(d)\n",
        "    del final_summary\n",
        "    del final_summary_sents\n",
        "    del best_model_summary\n",
        "    del candidate_sents\n",
        "    del query_doc\n",
        "\n",
        "    gc.collect()\n",
        "    \n",
        "    utils.write_jsonl(final_summary_list, final_summaries_path, override=True)\n",
        "  return final_summary_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mM8uIdrhVvo"
      },
      "source": [
        "from statistics import mean\n",
        "\n",
        "def score(final_summary_list):  \n",
        "  r1 = []\n",
        "  r2 = []\n",
        "  rl = []  \n",
        "\n",
        "  for c in final_summary_list:\n",
        "    rouge = c[\"rouge\"]\n",
        "    r1.append(rouge['rouge-1'])\n",
        "    r2.append(rouge['rouge-2'])\n",
        "    rl.append(rouge['rouge-l'])\n",
        "\n",
        "  r1p = [x['p'] for x in r1]\n",
        "  r1r = [x['r'] for x in r1]\n",
        "  r1f = [x['f'] for x in r1]\n",
        "  \n",
        "  r2p = [x['p'] for x in r2]\n",
        "  r2r = [x['r'] for x in r2]\n",
        "  r2f = [x['f'] for x in r2]\n",
        "  \n",
        "  rlp = [x['p'] for x in rl]\n",
        "  rlr = [x['r'] for x in rl]\n",
        "  rlf = [x['f'] for x in rl]\n",
        "\n",
        "  return (mean(r1p),mean(r1r),mean(r1f), mean(r2p),mean(r2r),mean(r2f), mean(rlp),mean(rlr),mean(rlf)) \n",
        "\n",
        "# print(\"r1 p: {:.3f} r: {:.3f} f: {:.3f} \\nr2 p: {:.3f} r: {:.3f} f: {:.3f} \\nrl p: {:.3f} r: {:.3f} f: {:.3f} \\n\".format(*score(final_summary_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JRBgh85T8VV"
      },
      "source": [
        "goodmodels = [\"LED\", \"BART\", \"T5\", \"Pegasus\", \"GPT2 EXT\", \"XLNet EXT\", \"BART EXT\"]\n",
        "\n",
        "lambda_constant =  0.9970352972330873\n",
        "mmr_percentage =  0.2979940927185918\n",
        "sim0 = process_doc2vec_similarity\n",
        "sim1 = process_word_movers_distance\n",
        "sim2 = process_tfidf_similarity\n",
        "MMR_initial_sents = True\n",
        "\n",
        "final_summary_list = generate_final_summaries(summaries, models = goodmodels, LDAtopics = topics,\n",
        "                                              test_data = test_data, lambda_constant = lambda_constant, \n",
        "                                              sim0 = sim0, sim1 = sim1, sim2 = sim2, \n",
        "                                              MMR_initial_sents = MMR_initial_sents, mmr_percentage = mmr_percentage,\n",
        "                                              bestModel = 'BART')\n",
        "\n",
        "score(final_summary_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0bPdXD32Yr5"
      },
      "source": [
        "## Hyperparameter optimisation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eEzSb3YJtE0"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyzOr9LxJrAl"
      },
      "source": [
        "import operator\n",
        "import numpy as np\n",
        "import optuna\n",
        "import random\n",
        "\n",
        "random.seed(10)\n",
        "goodmodels = [\"LED\", \"BART\", \"T5\", \"Pegasus\", \"GPT2 EXT\", \"XLNet EXT\", \"BART EXT\"]\n",
        "\n",
        "def objective(trial: optuna.Trial):\n",
        "\n",
        "  lambda_constant = trial.suggest_float('lambda_constant', 1e-5, 1.0, log=False)\n",
        "  num_topics = trial.suggest_int('num_topics', 1, 8)\n",
        "  num_words = trial.suggest_int('num_words', 1, 8)\n",
        "  mmr_percentage = trial.suggest_float('mmr_percentage', 0.0, 1.0, log=False)\n",
        "  sim1 = trial.suggest_categorical('sim1', ('process_tfidf_similarity', 'process_doc2vec_similarity', 'process_word_movers_distance'))\n",
        "  sim2 = trial.suggest_categorical('sim2', ('process_tfidf_similarity', 'process_doc2vec_similarity', 'process_word_movers_distance'))\n",
        "  sim0 = trial.suggest_categorical('sim0', ('process_tfidf_similarity', 'process_doc2vec_similarity', 'process_word_movers_distance'))\n",
        "  MMR_initial_sents = trial.suggest_categorical('MMR_initial_sents', (True,False))\n",
        "\n",
        "  if sim0 == 'process_tfidf_similarity':\n",
        "    sim0 = process_tfidf_similarity\n",
        "  elif sim0 == 'process_doc2vec_similarity':\n",
        "    sim0 = process_doc2vec_similarity\n",
        "  elif sim0 == 'process_word_movers_distance':\n",
        "    sim0 = process_word_movers_distance\n",
        "\n",
        "  if sim1 == 'process_tfidf_similarity':\n",
        "    sim1 = process_tfidf_similarity\n",
        "  elif sim1 == 'process_doc2vec_similarity':\n",
        "    sim1 = process_doc2vec_similarity\n",
        "  elif sim1 == 'process_word_movers_distance':\n",
        "    sim1 = process_word_movers_distance\n",
        "\n",
        "  if sim2 == 'process_tfidf_similarity':\n",
        "    sim2 = process_tfidf_similarity\n",
        "  elif sim2 == 'process_doc2vec_similarity':\n",
        "    sim2 = process_doc2vec_similarity\n",
        "  elif sim2 == 'process_word_movers_distance':\n",
        "    sim2 = process_word_movers_distance\n",
        "  \n",
        "  LDAtopics = LDA_topics(test_data = partial_test_data, num_topics = num_topics, num_words = num_words, num_articles=10)\n",
        "  summary_list = generate_final_summaries(summaries = summaries, models = goodmodels, LDAtopics = LDAtopics,\n",
        "                                              test_data = partial_test_data,lambda_constant = lambda_constant,\n",
        "                                              sim0 = sim0, sim1 = sim1, sim2 = sim2, \n",
        "                                              MMR_initial_sents = MMR_initial_sents, mmr_percentage = mmr_percentage,\n",
        "                                              bestModel = 'BART')\n",
        "  summary_score = score(summary_list)\n",
        "  \n",
        "  return summary_score[2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba9Ppc9HLbNY"
      },
      "source": [
        "import pickle\n",
        "direction = \"maximize\"\n",
        "study = optuna.create_study(study_name=\"\" ,direction=direction)\n",
        "study.optimize(objective, n_trials = 100)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/MMRSumm/outputs/optuna_study.pkl',\"wb\") as result:\n",
        "  pickle.dump(study, result, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(study.best_params)\n",
        "print(study.best_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tIS-ciatbuT"
      },
      "source": [
        "import pickle\n",
        "import optuna\n",
        "\n",
        "with open('/content/gdrive/MyDrive/MMRSumm/outputs/optuna_study.pkl', 'rb') as result:\n",
        "    study = pickle.load(result)\n",
        "\n",
        "print(study.best_params)\n",
        "print(study.best_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kml6dQuQJWo2"
      },
      "source": [
        "fig = optuna.visualization.plot_param_importances(study)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
